{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MILESTONE 2\n",
    "\n",
    "IMDB dataset + Siraj's Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset\n",
    "\n",
    "1. Removing punctuations\n",
    "2. Generating word_to_int map\n",
    "3. Coverting each review in ints\n",
    "4. Padding each review with 0's and generating input of length 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <PERIOD> ')\n",
    "    text = text.replace('\"', ' <PERIOD> ')\n",
    "    text = text.replace(';', ' <PERIOD> ')\n",
    "    text = text.replace('!', ' <PERIOD> ')\n",
    "    text = text.replace('?', ' <PERIOD> ')\n",
    "    text = text.replace('(', ' <PERIOD> ')\n",
    "    text = text.replace(')', ' <PERIOD> ')\n",
    "    text = text.replace('--', ' <PERIOD> ')\n",
    "    text = text.replace('?', ' <PERIOD> ')\n",
    "    '''\n",
    "    text = text.replace('<br />', ' <PERIOD> ')\n",
    "    text = text.replace('\\\\', ' <PERIOD> ')\n",
    "    text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <PERIOD> ')\n",
    "    text = text.replace(' <PERIOD> ', ' ')\n",
    "    words = text.split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "def removing_noise(words):\n",
    "    word_count = Counter(words)\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    words_new = [word for word in words if (word_count[word]>5) #and (not word in stops)\n",
    "                ]\n",
    "    return words_new\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'data/labeledTrainData.tsv'\n",
    "review_ids = []\n",
    "reviews = []\n",
    "labels = []\n",
    "#importing dataset into lists\n",
    "wrong_temp = []\n",
    "#[92, 102, 120, 259, 404, 1028, 1094, 1184, 1229, 1234, 1343, 1503, 1790, 1861, 2212, 3430, 3771, 3870, 4106, 4407, 4866, 5053, 5218, 5221, 5514, 5553, 5646, 5853, 6086, 6499, 6582, 6746, 7021, 7023, 7194, 7331, 7454, 7473, 7553, 7837, 8119, 8264, 8407, 8433, 8971, 9076, 9204, 9402, 9490, 9552, 9562, 9632, 9716, 9748, 9787, 10107, 10230, 10233, 10414, 10477, 10500, 10702, 10892, 11048, 11055, 11371, 11375, 11513, 11744, 11944, 12071, 12159, 12188, 12243, 12341, 12558, 12594, 12808, 13087, 13159, 14111, 14755, 14860, 14993, 15094, 15260, 15352, 15360, 15656, 15871, 16214, 16274, 16492, 16539, 16613, 16622, 16870, 16949, 16990, 16992, 17071, 17254, 17371, 17450, 17463, 17603, 17709, 17712, 17749, 18024, 18221, 18226, 18681, 18784, 18896, 19063, 19609, 19714, 19889]\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    row_count = 0\n",
    "    for row in reader:\n",
    "        review_ids.append(row[0])\n",
    "        labels.append([int(row[1])] )\n",
    "        reviews.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_pp = []\n",
    "words = []\n",
    "\n",
    "for review in reviews:\n",
    "    review_pp = preprocess(review)\n",
    "    reviews_pp.append(review_pp)\n",
    "    words.extend(review_pp)\n",
    "    \n",
    "words = removing_noise(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting word to integers and making the vocabulary\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "words_count = Counter(words)\n",
    "sorted_vocab = sorted(words_count, key = words_count.get, reverse = True)\n",
    "word_to_int = {word:i for i,word in enumerate(sorted_vocab,1)}\n",
    "\n",
    "#Converting each review in the form of integers\n",
    "reviews_pp_ints = []\n",
    "for review in reviews_pp:\n",
    "    this_review_int = []\n",
    "    for word in review:\n",
    "        if word in vocab:\n",
    "            this_review_int.append(word_to_int[word])\n",
    "    reviews_pp_ints.append(this_review_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp_ints[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_len = 200\n",
    "features = np.zeros((len(reviews_pp_ints), max_seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_pp_ints):\n",
    "    features[i, :len(row)] = np.array(row[:max_seq_len] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'features' is a 2d array storing all sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "embed_size = 100\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "hidden_nodes = 10\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'labels')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "tf.set_random_seed(5)\n",
    "\n",
    "embedding = tf.Variable(tf.random_uniform((vocab_size+1, embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers)\n",
    "\n",
    "#getting an initial state of zeros\\n\",\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state = initial_state)\n",
    "\n",
    "#hidden_layer = tf.contrib.layers.fully_connected(outputs[:, -1], hidden_nodes, activation_fn=tf.nn.relu)\n",
    "predictions = tf.contrib.layers.fully_connected(outputs[:, -1],1, activation_fn=tf.sigmoid)\n",
    "#predictions,Y\\n\",\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(Y - predictions))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy:\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.float32), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/60 Iteration: 5 Train loss: 0.37086\n",
      "Epoch: 0/60 Iteration: 10 Train loss: 0.26400\n",
      "Epoch: 0/60 Iteration: 15 Train loss: 0.26513\n",
      "Epoch: 0/60 Iteration: 20 Train loss: 0.25516\n",
      "Epoch: 0/60 Iteration: 25 Train loss: 0.26013\n",
      "Epoch: 0/60 Iteration: 30 Train loss: 0.25427\n",
      "Epoch: 0/60 Iteration: 35 Train loss: 0.25255\n",
      "Epoch: 0/60 Iteration: 40 Train loss: 0.24954\n",
      "Epoch: 1/60 Iteration: 5 Train loss: 0.24722\n",
      "Epoch: 1/60 Iteration: 10 Train loss: 0.24471\n",
      "Epoch: 1/60 Iteration: 15 Train loss: 0.25265\n",
      "Epoch: 1/60 Iteration: 20 Train loss: 0.24892\n",
      "Epoch: 1/60 Iteration: 25 Train loss: 0.24557\n",
      "Epoch: 1/60 Iteration: 30 Train loss: 0.24174\n",
      "Epoch: 1/60 Iteration: 35 Train loss: 0.24362\n",
      "Epoch: 1/60 Iteration: 40 Train loss: 0.24137\n",
      "Epoch: 2/60 Iteration: 5 Train loss: 0.24189\n",
      "Epoch: 2/60 Iteration: 10 Train loss: 0.23771\n",
      "Epoch: 2/60 Iteration: 15 Train loss: 0.23725\n",
      "Epoch: 2/60 Iteration: 20 Train loss: 0.22959\n",
      "Epoch: 2/60 Iteration: 25 Train loss: 0.22247\n",
      "Epoch: 2/60 Iteration: 30 Train loss: 0.19790\n",
      "Epoch: 2/60 Iteration: 35 Train loss: 0.21019\n",
      "Epoch: 2/60 Iteration: 40 Train loss: 0.21799\n",
      "Epoch: 3/60 Iteration: 5 Train loss: 0.21223\n",
      "Epoch: 3/60 Iteration: 10 Train loss: 0.20520\n",
      "Epoch: 3/60 Iteration: 15 Train loss: 0.21109\n",
      "Epoch: 3/60 Iteration: 20 Train loss: 0.19304\n",
      "Epoch: 3/60 Iteration: 25 Train loss: 0.17510\n",
      "Epoch: 3/60 Iteration: 30 Train loss: 0.16272\n",
      "Epoch: 3/60 Iteration: 35 Train loss: 0.16948\n",
      "Epoch: 3/60 Iteration: 40 Train loss: 0.16868\n",
      "Epoch: 4/60 Iteration: 5 Train loss: 0.14961\n",
      "Epoch: 4/60 Iteration: 10 Train loss: 0.13348\n",
      "Epoch: 4/60 Iteration: 15 Train loss: 0.19754\n",
      "Epoch: 4/60 Iteration: 20 Train loss: 0.17586\n",
      "Epoch: 4/60 Iteration: 25 Train loss: 0.15027\n",
      "Epoch: 4/60 Iteration: 30 Train loss: 0.14435\n",
      "Epoch: 4/60 Iteration: 35 Train loss: 0.12113\n",
      "Epoch: 4/60 Iteration: 40 Train loss: 0.20353\n",
      "Epoch: 5/60 Iteration: 5 Train loss: 0.14066\n",
      "Epoch: 5/60 Iteration: 10 Train loss: 0.12525\n",
      "Epoch: 5/60 Iteration: 15 Train loss: 0.12778\n",
      "Epoch: 5/60 Iteration: 20 Train loss: 0.14220\n",
      "Epoch: 5/60 Iteration: 25 Train loss: 0.14351\n",
      "Epoch: 5/60 Iteration: 30 Train loss: 0.12126\n",
      "Epoch: 5/60 Iteration: 35 Train loss: 0.07820\n",
      "Epoch: 5/60 Iteration: 40 Train loss: 0.14900\n",
      "Epoch: 6/60 Iteration: 5 Train loss: 0.07744\n",
      "Epoch: 6/60 Iteration: 10 Train loss: 0.07952\n",
      "Epoch: 6/60 Iteration: 15 Train loss: 0.11226\n",
      "Epoch: 6/60 Iteration: 20 Train loss: 0.12097\n",
      "Epoch: 6/60 Iteration: 25 Train loss: 0.12784\n",
      "Epoch: 6/60 Iteration: 30 Train loss: 0.09669\n",
      "Epoch: 6/60 Iteration: 35 Train loss: 0.10703\n",
      "Epoch: 6/60 Iteration: 40 Train loss: 0.09850\n",
      "Epoch: 7/60 Iteration: 5 Train loss: 0.06144\n",
      "Epoch: 7/60 Iteration: 10 Train loss: 0.06188\n",
      "Epoch: 7/60 Iteration: 15 Train loss: 0.06978\n",
      "Epoch: 7/60 Iteration: 20 Train loss: 0.08413\n",
      "Epoch: 7/60 Iteration: 25 Train loss: 0.05749\n",
      "Epoch: 7/60 Iteration: 30 Train loss: 0.05111\n",
      "Epoch: 7/60 Iteration: 35 Train loss: 0.04620\n",
      "Epoch: 7/60 Iteration: 40 Train loss: 0.06321\n",
      "Epoch: 8/60 Iteration: 5 Train loss: 0.04065\n",
      "Epoch: 8/60 Iteration: 10 Train loss: 0.04179\n",
      "Epoch: 8/60 Iteration: 15 Train loss: 0.04697\n",
      "Epoch: 8/60 Iteration: 20 Train loss: 0.07031\n",
      "Epoch: 8/60 Iteration: 25 Train loss: 0.05192\n",
      "Epoch: 8/60 Iteration: 30 Train loss: 0.03209\n",
      "Epoch: 8/60 Iteration: 35 Train loss: 0.03368\n",
      "Epoch: 8/60 Iteration: 40 Train loss: 0.04850\n",
      "Epoch: 9/60 Iteration: 5 Train loss: 0.02256\n",
      "Epoch: 9/60 Iteration: 10 Train loss: 0.04148\n",
      "Epoch: 9/60 Iteration: 15 Train loss: 0.03822\n",
      "Epoch: 9/60 Iteration: 20 Train loss: 0.04910\n",
      "Epoch: 9/60 Iteration: 25 Train loss: 0.03135\n",
      "Epoch: 9/60 Iteration: 30 Train loss: 0.04782\n",
      "Epoch: 9/60 Iteration: 35 Train loss: 0.04640\n",
      "Epoch: 9/60 Iteration: 40 Train loss: 0.04321\n",
      "Epoch: 10/60 Iteration: 5 Train loss: 0.02604\n",
      "Epoch: 10/60 Iteration: 10 Train loss: 0.03053\n",
      "Epoch: 10/60 Iteration: 15 Train loss: 0.03046\n",
      "Epoch: 10/60 Iteration: 20 Train loss: 0.04948\n",
      "Epoch: 10/60 Iteration: 25 Train loss: 0.02813\n",
      "Epoch: 10/60 Iteration: 30 Train loss: 0.02657\n",
      "Epoch: 10/60 Iteration: 35 Train loss: 0.02803\n",
      "Epoch: 10/60 Iteration: 40 Train loss: 0.07053\n",
      "Epoch: 11/60 Iteration: 5 Train loss: 0.01874\n",
      "Epoch: 11/60 Iteration: 10 Train loss: 0.03372\n",
      "Epoch: 11/60 Iteration: 15 Train loss: 0.02789\n",
      "Epoch: 11/60 Iteration: 20 Train loss: 0.03528\n",
      "Epoch: 11/60 Iteration: 25 Train loss: 0.02418\n",
      "Epoch: 11/60 Iteration: 30 Train loss: 0.02120\n",
      "Epoch: 11/60 Iteration: 35 Train loss: 0.02189\n",
      "Epoch: 11/60 Iteration: 40 Train loss: 0.02907\n",
      "Epoch: 12/60 Iteration: 5 Train loss: 0.01249\n",
      "Epoch: 12/60 Iteration: 10 Train loss: 0.02198\n",
      "Epoch: 12/60 Iteration: 15 Train loss: 0.02274\n",
      "Epoch: 12/60 Iteration: 20 Train loss: 0.03097\n",
      "Epoch: 12/60 Iteration: 25 Train loss: 0.01799\n",
      "Epoch: 12/60 Iteration: 30 Train loss: 0.02620\n",
      "Epoch: 12/60 Iteration: 35 Train loss: 0.01931\n",
      "Epoch: 12/60 Iteration: 40 Train loss: 0.04471\n",
      "Epoch: 13/60 Iteration: 5 Train loss: 0.01094\n",
      "Epoch: 13/60 Iteration: 10 Train loss: 0.02453\n",
      "Epoch: 13/60 Iteration: 15 Train loss: 0.01845\n",
      "Epoch: 13/60 Iteration: 20 Train loss: 0.02878\n",
      "Epoch: 13/60 Iteration: 25 Train loss: 0.02630\n",
      "Epoch: 13/60 Iteration: 30 Train loss: 0.02249\n",
      "Epoch: 13/60 Iteration: 35 Train loss: 0.02051\n",
      "Epoch: 13/60 Iteration: 40 Train loss: 0.03389\n",
      "Epoch: 14/60 Iteration: 5 Train loss: 0.02917\n",
      "Epoch: 14/60 Iteration: 10 Train loss: 0.01943\n",
      "Epoch: 14/60 Iteration: 15 Train loss: 0.09249\n",
      "Epoch: 14/60 Iteration: 20 Train loss: 0.09864\n",
      "Epoch: 14/60 Iteration: 25 Train loss: 0.02641\n",
      "Epoch: 14/60 Iteration: 30 Train loss: 0.04878\n",
      "Epoch: 14/60 Iteration: 35 Train loss: 0.02749\n",
      "Epoch: 14/60 Iteration: 40 Train loss: 0.03018\n",
      "Epoch: 15/60 Iteration: 5 Train loss: 0.01625\n",
      "Epoch: 15/60 Iteration: 10 Train loss: 0.01853\n",
      "Epoch: 15/60 Iteration: 15 Train loss: 0.02769\n",
      "Epoch: 15/60 Iteration: 20 Train loss: 0.03701\n",
      "Epoch: 15/60 Iteration: 25 Train loss: 0.04150\n",
      "Epoch: 15/60 Iteration: 30 Train loss: 0.01332\n",
      "Epoch: 15/60 Iteration: 35 Train loss: 0.02857\n",
      "Epoch: 15/60 Iteration: 40 Train loss: 0.02413\n",
      "Epoch: 16/60 Iteration: 5 Train loss: 0.02690\n",
      "Epoch: 16/60 Iteration: 10 Train loss: 0.02450\n",
      "Epoch: 16/60 Iteration: 15 Train loss: 0.01857\n",
      "Epoch: 16/60 Iteration: 20 Train loss: 0.05142\n",
      "Epoch: 16/60 Iteration: 25 Train loss: 0.01071\n",
      "Epoch: 16/60 Iteration: 30 Train loss: 0.01957\n",
      "Epoch: 16/60 Iteration: 35 Train loss: 0.01726\n",
      "Epoch: 16/60 Iteration: 40 Train loss: 0.02096\n",
      "Epoch: 17/60 Iteration: 5 Train loss: 0.01114\n",
      "Epoch: 17/60 Iteration: 10 Train loss: 0.01806\n",
      "Epoch: 17/60 Iteration: 15 Train loss: 0.01792\n",
      "Epoch: 17/60 Iteration: 20 Train loss: 0.03777\n",
      "Epoch: 17/60 Iteration: 25 Train loss: 0.01945\n",
      "Epoch: 17/60 Iteration: 30 Train loss: 0.01171\n",
      "Epoch: 17/60 Iteration: 35 Train loss: 0.01520\n",
      "Epoch: 17/60 Iteration: 40 Train loss: 0.01590\n",
      "Epoch: 18/60 Iteration: 5 Train loss: 0.00688\n",
      "Epoch: 18/60 Iteration: 10 Train loss: 0.01112\n",
      "Epoch: 18/60 Iteration: 15 Train loss: 0.01848\n",
      "Epoch: 18/60 Iteration: 20 Train loss: 0.02174\n",
      "Epoch: 18/60 Iteration: 25 Train loss: 0.00799\n",
      "Epoch: 18/60 Iteration: 30 Train loss: 0.01195\n",
      "Epoch: 18/60 Iteration: 35 Train loss: 0.01185\n",
      "Epoch: 18/60 Iteration: 40 Train loss: 0.01709\n",
      "Epoch: 19/60 Iteration: 5 Train loss: 0.00923\n",
      "Epoch: 19/60 Iteration: 10 Train loss: 0.01117\n",
      "Epoch: 19/60 Iteration: 15 Train loss: 0.01740\n",
      "Epoch: 19/60 Iteration: 20 Train loss: 0.01932\n",
      "Epoch: 19/60 Iteration: 25 Train loss: 0.00569\n",
      "Epoch: 19/60 Iteration: 30 Train loss: 0.00617\n",
      "Epoch: 19/60 Iteration: 35 Train loss: 0.01067\n",
      "Epoch: 19/60 Iteration: 40 Train loss: 0.01198\n",
      "Epoch: 20/60 Iteration: 5 Train loss: 0.00446\n",
      "Epoch: 20/60 Iteration: 10 Train loss: 0.01112\n",
      "Epoch: 20/60 Iteration: 15 Train loss: 0.01182\n",
      "Epoch: 20/60 Iteration: 20 Train loss: 0.01642\n",
      "Epoch: 20/60 Iteration: 25 Train loss: 0.00281\n",
      "Epoch: 20/60 Iteration: 30 Train loss: 0.00622\n",
      "Epoch: 20/60 Iteration: 35 Train loss: 0.01011\n",
      "Epoch: 20/60 Iteration: 40 Train loss: 0.01177\n",
      "Epoch: 21/60 Iteration: 5 Train loss: 0.00444\n",
      "Epoch: 21/60 Iteration: 10 Train loss: 0.01013\n",
      "Epoch: 21/60 Iteration: 15 Train loss: 0.01179\n",
      "Epoch: 21/60 Iteration: 20 Train loss: 0.01454\n",
      "Epoch: 21/60 Iteration: 25 Train loss: 0.00225\n",
      "Epoch: 21/60 Iteration: 30 Train loss: 0.00808\n",
      "Epoch: 21/60 Iteration: 35 Train loss: 0.01046\n",
      "Epoch: 21/60 Iteration: 40 Train loss: 0.01374\n",
      "Epoch: 22/60 Iteration: 5 Train loss: 0.00377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/60 Iteration: 10 Train loss: 0.01045\n",
      "Epoch: 22/60 Iteration: 15 Train loss: 0.01387\n",
      "Epoch: 22/60 Iteration: 20 Train loss: 0.01337\n",
      "Epoch: 22/60 Iteration: 25 Train loss: 0.00240\n",
      "Epoch: 22/60 Iteration: 30 Train loss: 0.00616\n",
      "Epoch: 22/60 Iteration: 35 Train loss: 0.01071\n",
      "Epoch: 22/60 Iteration: 40 Train loss: 0.01181\n",
      "Epoch: 23/60 Iteration: 5 Train loss: 0.00213\n",
      "Epoch: 23/60 Iteration: 10 Train loss: 0.00999\n",
      "Epoch: 23/60 Iteration: 15 Train loss: 0.00995\n",
      "Epoch: 23/60 Iteration: 20 Train loss: 0.01387\n",
      "Epoch: 23/60 Iteration: 25 Train loss: 0.00208\n",
      "Epoch: 23/60 Iteration: 30 Train loss: 0.00803\n",
      "Epoch: 23/60 Iteration: 35 Train loss: 0.00802\n",
      "Epoch: 23/60 Iteration: 40 Train loss: 0.01188\n",
      "Epoch: 24/60 Iteration: 5 Train loss: 0.00203\n",
      "Epoch: 24/60 Iteration: 10 Train loss: 0.01080\n",
      "Epoch: 24/60 Iteration: 15 Train loss: 0.01152\n",
      "Epoch: 24/60 Iteration: 20 Train loss: 0.01090\n",
      "Epoch: 24/60 Iteration: 25 Train loss: 0.00580\n",
      "Epoch: 24/60 Iteration: 30 Train loss: 0.00617\n",
      "Epoch: 24/60 Iteration: 35 Train loss: 0.00987\n",
      "Epoch: 24/60 Iteration: 40 Train loss: 0.01158\n",
      "Epoch: 25/60 Iteration: 5 Train loss: 0.00219\n",
      "Epoch: 25/60 Iteration: 10 Train loss: 0.01140\n",
      "Epoch: 25/60 Iteration: 15 Train loss: 0.01025\n",
      "Epoch: 25/60 Iteration: 20 Train loss: 0.01681\n",
      "Epoch: 25/60 Iteration: 25 Train loss: 0.00222\n",
      "Epoch: 25/60 Iteration: 30 Train loss: 0.00836\n",
      "Epoch: 25/60 Iteration: 35 Train loss: 0.00800\n",
      "Epoch: 25/60 Iteration: 40 Train loss: 0.01018\n",
      "Epoch: 26/60 Iteration: 5 Train loss: 0.00213\n",
      "Epoch: 26/60 Iteration: 10 Train loss: 0.00989\n",
      "Epoch: 26/60 Iteration: 15 Train loss: 0.00612\n",
      "Epoch: 26/60 Iteration: 20 Train loss: 0.01596\n",
      "Epoch: 26/60 Iteration: 25 Train loss: 0.00302\n",
      "Epoch: 26/60 Iteration: 30 Train loss: 0.00617\n",
      "Epoch: 26/60 Iteration: 35 Train loss: 0.00931\n",
      "Epoch: 26/60 Iteration: 40 Train loss: 0.00867\n",
      "Epoch: 27/60 Iteration: 5 Train loss: 0.00216\n",
      "Epoch: 27/60 Iteration: 10 Train loss: 0.01043\n",
      "Epoch: 27/60 Iteration: 15 Train loss: 0.00999\n",
      "Epoch: 27/60 Iteration: 20 Train loss: 0.01160\n",
      "Epoch: 27/60 Iteration: 25 Train loss: 0.00242\n",
      "Epoch: 27/60 Iteration: 30 Train loss: 0.00628\n",
      "Epoch: 27/60 Iteration: 35 Train loss: 0.00800\n",
      "Epoch: 27/60 Iteration: 40 Train loss: 0.00791\n",
      "Epoch: 28/60 Iteration: 5 Train loss: 0.00205\n",
      "Epoch: 28/60 Iteration: 10 Train loss: 0.00802\n",
      "Epoch: 28/60 Iteration: 15 Train loss: 0.00787\n",
      "Epoch: 28/60 Iteration: 20 Train loss: 0.01069\n",
      "Epoch: 28/60 Iteration: 25 Train loss: 0.00230\n",
      "Epoch: 28/60 Iteration: 30 Train loss: 0.00617\n",
      "Epoch: 28/60 Iteration: 35 Train loss: 0.00797\n",
      "Epoch: 28/60 Iteration: 40 Train loss: 0.00762\n",
      "Epoch: 29/60 Iteration: 5 Train loss: 0.00212\n",
      "Epoch: 29/60 Iteration: 10 Train loss: 0.00606\n",
      "Epoch: 29/60 Iteration: 15 Train loss: 0.00618\n",
      "Epoch: 29/60 Iteration: 20 Train loss: 0.01021\n",
      "Epoch: 29/60 Iteration: 25 Train loss: 0.00216\n",
      "Epoch: 29/60 Iteration: 30 Train loss: 0.00630\n",
      "Epoch: 29/60 Iteration: 35 Train loss: 0.00789\n",
      "Epoch: 29/60 Iteration: 40 Train loss: 0.00600\n",
      "Epoch: 30/60 Iteration: 5 Train loss: 0.00376\n",
      "Epoch: 30/60 Iteration: 10 Train loss: 0.00691\n",
      "Epoch: 30/60 Iteration: 15 Train loss: 0.00499\n",
      "Epoch: 30/60 Iteration: 20 Train loss: 0.01181\n",
      "Epoch: 30/60 Iteration: 25 Train loss: 0.00339\n",
      "Epoch: 30/60 Iteration: 30 Train loss: 0.00613\n",
      "Epoch: 30/60 Iteration: 35 Train loss: 0.00972\n",
      "Epoch: 30/60 Iteration: 40 Train loss: 0.00530\n",
      "Epoch: 31/60 Iteration: 5 Train loss: 0.00222\n",
      "Epoch: 31/60 Iteration: 10 Train loss: 0.00840\n",
      "Epoch: 31/60 Iteration: 15 Train loss: 0.00674\n",
      "Epoch: 31/60 Iteration: 20 Train loss: 0.01301\n",
      "Epoch: 31/60 Iteration: 25 Train loss: 0.00630\n",
      "Epoch: 31/60 Iteration: 30 Train loss: 0.00660\n",
      "Epoch: 31/60 Iteration: 35 Train loss: 0.00824\n",
      "Epoch: 31/60 Iteration: 40 Train loss: 0.00845\n",
      "Epoch: 32/60 Iteration: 5 Train loss: 0.00262\n",
      "Epoch: 32/60 Iteration: 10 Train loss: 0.00751\n",
      "Epoch: 32/60 Iteration: 15 Train loss: 0.00680\n",
      "Epoch: 32/60 Iteration: 20 Train loss: 0.01634\n",
      "Epoch: 32/60 Iteration: 25 Train loss: 0.00208\n",
      "Epoch: 32/60 Iteration: 30 Train loss: 0.00665\n",
      "Epoch: 32/60 Iteration: 35 Train loss: 0.00868\n",
      "Epoch: 32/60 Iteration: 40 Train loss: 0.00614\n",
      "Epoch: 33/60 Iteration: 5 Train loss: 0.00203\n",
      "Epoch: 33/60 Iteration: 10 Train loss: 0.00825\n",
      "Epoch: 33/60 Iteration: 15 Train loss: 0.00597\n",
      "Epoch: 33/60 Iteration: 20 Train loss: 0.00972\n",
      "Epoch: 33/60 Iteration: 25 Train loss: 0.00746\n",
      "Epoch: 33/60 Iteration: 30 Train loss: 0.00694\n",
      "Epoch: 33/60 Iteration: 35 Train loss: 0.00801\n",
      "Epoch: 33/60 Iteration: 40 Train loss: 0.00800\n",
      "Epoch: 34/60 Iteration: 5 Train loss: 0.00420\n",
      "Epoch: 34/60 Iteration: 10 Train loss: 0.00902\n",
      "Epoch: 34/60 Iteration: 15 Train loss: 0.00409\n",
      "Epoch: 34/60 Iteration: 20 Train loss: 0.01328\n",
      "Epoch: 34/60 Iteration: 25 Train loss: 0.00424\n",
      "Epoch: 34/60 Iteration: 30 Train loss: 0.00807\n",
      "Epoch: 34/60 Iteration: 35 Train loss: 0.01251\n",
      "Epoch: 34/60 Iteration: 40 Train loss: 0.00761\n",
      "Epoch: 35/60 Iteration: 5 Train loss: 0.00252\n",
      "Epoch: 35/60 Iteration: 10 Train loss: 0.00655\n",
      "Epoch: 35/60 Iteration: 15 Train loss: 0.00849\n",
      "Epoch: 35/60 Iteration: 20 Train loss: 0.00832\n",
      "Epoch: 35/60 Iteration: 25 Train loss: 0.00530\n",
      "Epoch: 35/60 Iteration: 30 Train loss: 0.00759\n",
      "Epoch: 35/60 Iteration: 35 Train loss: 0.00794\n",
      "Epoch: 35/60 Iteration: 40 Train loss: 0.00598\n",
      "Epoch: 36/60 Iteration: 5 Train loss: 0.00318\n",
      "Epoch: 36/60 Iteration: 10 Train loss: 0.00607\n",
      "Epoch: 36/60 Iteration: 15 Train loss: 0.00664\n",
      "Epoch: 36/60 Iteration: 20 Train loss: 0.00873\n",
      "Epoch: 36/60 Iteration: 25 Train loss: 0.00620\n",
      "Epoch: 36/60 Iteration: 30 Train loss: 0.00610\n",
      "Epoch: 36/60 Iteration: 35 Train loss: 0.00813\n",
      "Epoch: 36/60 Iteration: 40 Train loss: 0.00595\n",
      "Epoch: 37/60 Iteration: 5 Train loss: 0.00447\n",
      "Epoch: 37/60 Iteration: 10 Train loss: 0.00720\n",
      "Epoch: 37/60 Iteration: 15 Train loss: 0.00496\n",
      "Epoch: 37/60 Iteration: 20 Train loss: 0.00940\n",
      "Epoch: 37/60 Iteration: 25 Train loss: 0.00427\n",
      "Epoch: 37/60 Iteration: 30 Train loss: 0.00605\n",
      "Epoch: 37/60 Iteration: 35 Train loss: 0.00603\n",
      "Epoch: 37/60 Iteration: 40 Train loss: 0.00742\n",
      "Epoch: 38/60 Iteration: 5 Train loss: 0.00268\n",
      "Epoch: 38/60 Iteration: 10 Train loss: 0.00634\n",
      "Epoch: 38/60 Iteration: 15 Train loss: 0.00708\n",
      "Epoch: 38/60 Iteration: 20 Train loss: 0.00916\n",
      "Epoch: 38/60 Iteration: 25 Train loss: 0.00445\n",
      "Epoch: 38/60 Iteration: 30 Train loss: 0.00641\n",
      "Epoch: 38/60 Iteration: 35 Train loss: 0.00797\n",
      "Epoch: 38/60 Iteration: 40 Train loss: 0.00721\n",
      "Epoch: 39/60 Iteration: 5 Train loss: 0.00412\n",
      "Epoch: 39/60 Iteration: 10 Train loss: 0.00621\n",
      "Epoch: 39/60 Iteration: 15 Train loss: 0.00608\n",
      "Epoch: 39/60 Iteration: 20 Train loss: 0.00843\n",
      "Epoch: 39/60 Iteration: 25 Train loss: 0.00605\n",
      "Epoch: 39/60 Iteration: 30 Train loss: 0.00809\n",
      "Epoch: 39/60 Iteration: 35 Train loss: 0.00709\n",
      "Epoch: 39/60 Iteration: 40 Train loss: 0.00589\n",
      "Epoch: 40/60 Iteration: 5 Train loss: 0.00451\n",
      "Epoch: 40/60 Iteration: 10 Train loss: 0.01376\n",
      "Epoch: 40/60 Iteration: 15 Train loss: 0.00993\n",
      "Epoch: 40/60 Iteration: 20 Train loss: 0.00823\n",
      "Epoch: 40/60 Iteration: 25 Train loss: 0.00721\n",
      "Epoch: 40/60 Iteration: 30 Train loss: 0.00626\n",
      "Epoch: 40/60 Iteration: 35 Train loss: 0.00817\n",
      "Epoch: 40/60 Iteration: 40 Train loss: 0.00614\n",
      "Epoch: 41/60 Iteration: 5 Train loss: 0.00216\n",
      "Epoch: 41/60 Iteration: 10 Train loss: 0.00657\n",
      "Epoch: 41/60 Iteration: 15 Train loss: 0.00627\n",
      "Epoch: 41/60 Iteration: 20 Train loss: 0.01424\n",
      "Epoch: 41/60 Iteration: 25 Train loss: 0.00440\n",
      "Epoch: 41/60 Iteration: 30 Train loss: 0.01927\n",
      "Epoch: 41/60 Iteration: 35 Train loss: 0.01100\n",
      "Epoch: 41/60 Iteration: 40 Train loss: 0.01682\n",
      "Epoch: 42/60 Iteration: 5 Train loss: 0.00973\n",
      "Epoch: 42/60 Iteration: 10 Train loss: 0.01217\n",
      "Epoch: 42/60 Iteration: 15 Train loss: 0.00729\n",
      "Epoch: 42/60 Iteration: 20 Train loss: 0.01219\n",
      "Epoch: 42/60 Iteration: 25 Train loss: 0.00262\n",
      "Epoch: 42/60 Iteration: 30 Train loss: 0.01982\n",
      "Epoch: 42/60 Iteration: 35 Train loss: 0.00925\n",
      "Epoch: 42/60 Iteration: 40 Train loss: 0.00905\n",
      "Epoch: 43/60 Iteration: 5 Train loss: 0.00517\n",
      "Epoch: 43/60 Iteration: 10 Train loss: 0.00770\n",
      "Epoch: 43/60 Iteration: 15 Train loss: 0.00822\n",
      "Epoch: 43/60 Iteration: 20 Train loss: 0.00954\n",
      "Epoch: 43/60 Iteration: 25 Train loss: 0.00742\n",
      "Epoch: 43/60 Iteration: 30 Train loss: 0.01229\n",
      "Epoch: 43/60 Iteration: 35 Train loss: 0.01088\n",
      "Epoch: 43/60 Iteration: 40 Train loss: 0.01346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/60 Iteration: 5 Train loss: 0.00424\n",
      "Epoch: 44/60 Iteration: 10 Train loss: 0.00690\n",
      "Epoch: 44/60 Iteration: 15 Train loss: 0.01074\n",
      "Epoch: 44/60 Iteration: 20 Train loss: 0.01037\n",
      "Epoch: 44/60 Iteration: 25 Train loss: 0.00943\n",
      "Epoch: 44/60 Iteration: 30 Train loss: 0.00921\n",
      "Epoch: 44/60 Iteration: 35 Train loss: 0.00857\n",
      "Epoch: 44/60 Iteration: 40 Train loss: 0.00947\n",
      "Epoch: 45/60 Iteration: 5 Train loss: 0.00393\n",
      "Epoch: 45/60 Iteration: 10 Train loss: 0.00925\n",
      "Epoch: 45/60 Iteration: 15 Train loss: 0.00993\n",
      "Epoch: 45/60 Iteration: 20 Train loss: 0.01069\n",
      "Epoch: 45/60 Iteration: 25 Train loss: 0.01130\n",
      "Epoch: 45/60 Iteration: 30 Train loss: 0.01192\n",
      "Epoch: 45/60 Iteration: 35 Train loss: 0.00796\n",
      "Epoch: 45/60 Iteration: 40 Train loss: 0.01454\n",
      "Epoch: 46/60 Iteration: 5 Train loss: 0.00418\n",
      "Epoch: 46/60 Iteration: 10 Train loss: 0.00765\n",
      "Epoch: 46/60 Iteration: 15 Train loss: 0.00945\n",
      "Epoch: 46/60 Iteration: 20 Train loss: 0.00820\n",
      "Epoch: 46/60 Iteration: 25 Train loss: 0.01192\n",
      "Epoch: 46/60 Iteration: 30 Train loss: 0.00943\n",
      "Epoch: 46/60 Iteration: 35 Train loss: 0.01234\n",
      "Epoch: 46/60 Iteration: 40 Train loss: 0.00614\n",
      "Epoch: 47/60 Iteration: 5 Train loss: 0.01147\n",
      "Epoch: 47/60 Iteration: 10 Train loss: 0.00758\n",
      "Epoch: 47/60 Iteration: 15 Train loss: 0.00651\n",
      "Epoch: 47/60 Iteration: 20 Train loss: 0.00870\n",
      "Epoch: 47/60 Iteration: 25 Train loss: 0.00426\n",
      "Epoch: 47/60 Iteration: 30 Train loss: 0.01452\n",
      "Epoch: 47/60 Iteration: 35 Train loss: 0.00659\n",
      "Epoch: 47/60 Iteration: 40 Train loss: 0.01184\n",
      "Epoch: 48/60 Iteration: 5 Train loss: 0.00491\n",
      "Epoch: 48/60 Iteration: 10 Train loss: 0.00746\n",
      "Epoch: 48/60 Iteration: 15 Train loss: 0.00682\n",
      "Epoch: 48/60 Iteration: 20 Train loss: 0.01079\n",
      "Epoch: 48/60 Iteration: 25 Train loss: 0.00830\n",
      "Epoch: 48/60 Iteration: 30 Train loss: 0.00876\n",
      "Epoch: 48/60 Iteration: 35 Train loss: 0.00513\n",
      "Epoch: 48/60 Iteration: 40 Train loss: 0.00798\n",
      "Epoch: 49/60 Iteration: 5 Train loss: 0.00343\n",
      "Epoch: 49/60 Iteration: 10 Train loss: 0.00609\n",
      "Epoch: 49/60 Iteration: 15 Train loss: 0.00450\n",
      "Epoch: 49/60 Iteration: 20 Train loss: 0.00888\n",
      "Epoch: 49/60 Iteration: 25 Train loss: 0.00439\n",
      "Epoch: 49/60 Iteration: 30 Train loss: 0.00867\n",
      "Epoch: 49/60 Iteration: 35 Train loss: 0.00417\n",
      "Epoch: 49/60 Iteration: 40 Train loss: 0.00851\n",
      "Epoch: 50/60 Iteration: 5 Train loss: 0.00264\n",
      "Epoch: 50/60 Iteration: 10 Train loss: 0.00607\n",
      "Epoch: 50/60 Iteration: 15 Train loss: 0.00555\n",
      "Epoch: 50/60 Iteration: 20 Train loss: 0.01026\n",
      "Epoch: 50/60 Iteration: 25 Train loss: 0.00500\n",
      "Epoch: 50/60 Iteration: 30 Train loss: 0.01140\n",
      "Epoch: 50/60 Iteration: 35 Train loss: 0.00422\n",
      "Epoch: 50/60 Iteration: 40 Train loss: 0.00742\n",
      "Epoch: 51/60 Iteration: 5 Train loss: 0.00406\n",
      "Epoch: 51/60 Iteration: 10 Train loss: 0.00732\n",
      "Epoch: 51/60 Iteration: 15 Train loss: 0.00431\n",
      "Epoch: 51/60 Iteration: 20 Train loss: 0.01131\n",
      "Epoch: 51/60 Iteration: 25 Train loss: 0.00199\n",
      "Epoch: 51/60 Iteration: 30 Train loss: 0.00860\n",
      "Epoch: 51/60 Iteration: 35 Train loss: 0.00603\n",
      "Epoch: 51/60 Iteration: 40 Train loss: 0.01109\n",
      "Epoch: 52/60 Iteration: 5 Train loss: 0.00208\n",
      "Epoch: 52/60 Iteration: 10 Train loss: 0.00613\n",
      "Epoch: 52/60 Iteration: 15 Train loss: 0.00493\n",
      "Epoch: 52/60 Iteration: 20 Train loss: 0.00798\n",
      "Epoch: 52/60 Iteration: 25 Train loss: 0.00202\n",
      "Epoch: 52/60 Iteration: 30 Train loss: 0.00820\n",
      "Epoch: 52/60 Iteration: 35 Train loss: 0.00922\n",
      "Epoch: 52/60 Iteration: 40 Train loss: 0.00785\n",
      "Epoch: 53/60 Iteration: 5 Train loss: 0.00580\n",
      "Epoch: 53/60 Iteration: 10 Train loss: 0.00602\n",
      "Epoch: 53/60 Iteration: 15 Train loss: 0.00873\n",
      "Epoch: 53/60 Iteration: 20 Train loss: 0.00903\n",
      "Epoch: 53/60 Iteration: 25 Train loss: 0.00617\n",
      "Epoch: 53/60 Iteration: 30 Train loss: 0.00993\n",
      "Epoch: 53/60 Iteration: 35 Train loss: 0.00432\n",
      "Epoch: 53/60 Iteration: 40 Train loss: 0.00574\n",
      "Epoch: 54/60 Iteration: 5 Train loss: 0.00376\n",
      "Epoch: 54/60 Iteration: 10 Train loss: 0.00825\n",
      "Epoch: 54/60 Iteration: 15 Train loss: 0.00435\n",
      "Epoch: 54/60 Iteration: 20 Train loss: 0.01172\n",
      "Epoch: 54/60 Iteration: 25 Train loss: 0.00225\n",
      "Epoch: 54/60 Iteration: 30 Train loss: 0.01138\n",
      "Epoch: 54/60 Iteration: 35 Train loss: 0.00550\n",
      "Epoch: 54/60 Iteration: 40 Train loss: 0.00579\n",
      "Epoch: 55/60 Iteration: 5 Train loss: 0.00352\n",
      "Epoch: 55/60 Iteration: 10 Train loss: 0.00871\n",
      "Epoch: 55/60 Iteration: 15 Train loss: 0.00668\n",
      "Epoch: 55/60 Iteration: 20 Train loss: 0.00678\n",
      "Epoch: 55/60 Iteration: 25 Train loss: 0.00253\n",
      "Epoch: 55/60 Iteration: 30 Train loss: 0.00795\n",
      "Epoch: 55/60 Iteration: 35 Train loss: 0.00620\n",
      "Epoch: 55/60 Iteration: 40 Train loss: 0.00590\n",
      "Epoch: 56/60 Iteration: 5 Train loss: 0.00223\n",
      "Epoch: 56/60 Iteration: 10 Train loss: 0.00951\n",
      "Epoch: 56/60 Iteration: 15 Train loss: 0.00414\n",
      "Epoch: 56/60 Iteration: 20 Train loss: 0.00813\n",
      "Epoch: 56/60 Iteration: 25 Train loss: 0.00145\n",
      "Epoch: 56/60 Iteration: 30 Train loss: 0.00951\n",
      "Epoch: 56/60 Iteration: 35 Train loss: 0.00586\n",
      "Epoch: 56/60 Iteration: 40 Train loss: 0.00607\n",
      "Epoch: 57/60 Iteration: 5 Train loss: 0.00208\n",
      "Epoch: 57/60 Iteration: 10 Train loss: 0.00602\n",
      "Epoch: 57/60 Iteration: 15 Train loss: 0.00525\n",
      "Epoch: 57/60 Iteration: 20 Train loss: 0.01103\n",
      "Epoch: 57/60 Iteration: 25 Train loss: 0.00265\n",
      "Epoch: 57/60 Iteration: 30 Train loss: 0.00808\n",
      "Epoch: 57/60 Iteration: 35 Train loss: 0.01671\n",
      "Epoch: 57/60 Iteration: 40 Train loss: 0.00601\n",
      "Epoch: 58/60 Iteration: 5 Train loss: 0.00205\n",
      "Epoch: 58/60 Iteration: 10 Train loss: 0.00809\n",
      "Epoch: 58/60 Iteration: 15 Train loss: 0.01241\n",
      "Epoch: 58/60 Iteration: 20 Train loss: 0.01275\n",
      "Epoch: 58/60 Iteration: 25 Train loss: 0.00529\n",
      "Epoch: 58/60 Iteration: 30 Train loss: 0.01160\n",
      "Epoch: 58/60 Iteration: 35 Train loss: 0.00853\n",
      "Epoch: 58/60 Iteration: 40 Train loss: 0.00421\n",
      "Epoch: 59/60 Iteration: 5 Train loss: 0.00593\n",
      "Epoch: 59/60 Iteration: 10 Train loss: 0.00856\n",
      "Epoch: 59/60 Iteration: 15 Train loss: 0.00769\n",
      "Epoch: 59/60 Iteration: 20 Train loss: 0.01441\n",
      "Epoch: 59/60 Iteration: 25 Train loss: 0.00638\n",
      "Epoch: 59/60 Iteration: 30 Train loss: 0.01565\n",
      "Epoch: 59/60 Iteration: 35 Train loss: 0.00675\n",
      "Epoch: 59/60 Iteration: 40 Train loss: 0.00457\n",
      "Training Completed\n",
      "Total Time Taken: 958.3868825435638 sec\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "global_train_acc = []\n",
    "global_test_acc = []\n",
    "\n",
    "global_train_loss = []\n",
    "global_test_loss = []\n",
    "import time\n",
    "start_time = time.time()\n",
    "for e in range(n_epochs):\n",
    "    state = sess.run(initial_state)\n",
    "    iteration = 1\n",
    "    loss_=0.0\n",
    "    temp_train_loss = []\n",
    "    for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "        feed = {X: x, Y: y, initial_state: state, keep_prob: 0.5}\n",
    "\n",
    "        state, loss_,  _ = sess.run([final_state, loss, optimizer], feed_dict=feed)\n",
    "\n",
    "        if iteration%5==0:\n",
    "            print(\"Epoch: {}/{}\".format(e, n_epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Train loss: {:.5f}\".format(loss_))\n",
    "        temp_train_loss.append(loss_)\n",
    "        '''\n",
    "        if iteration%25==0:\n",
    "            val_acc = []\n",
    "            val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                feed = {X: x,\n",
    "                        Y: y,\n",
    "                        initial_state: val_state}\n",
    "                batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                val_acc.append(batch_acc)\n",
    "            print(\"Val acc: {:.5f}\".format(np.mean(val_acc)))\n",
    "        '''\n",
    "        '''\n",
    "        if iteration%25==0:\n",
    "            # train Acc calculation\n",
    "            train_acc = []\n",
    "            train_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for x, y in get_batches(train_x, train_y, batch_size):\n",
    "                feed = {X: x,\n",
    "                        Y: y,\n",
    "                        initial_state: train_state}\n",
    "                batch_acc, train_state, corr = sess.run([accuracy, final_state, correct_pred], feed_dict=feed)\n",
    "                bad_indexes = [index for index, correctness in enumerate(corr) if correctness ==0 ]\n",
    "                train_acc.append(batch_acc)\n",
    "            print(\"Train acc: {:.5f}\".format(np.mean(train_acc)))\n",
    "            global_train_acc.append(np.mean(train_acc))\n",
    "            \n",
    "            # test acc calculation\n",
    "            test_acc = []\n",
    "            test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "                feed = {X: x,Y: y,initial_state: test_state}\n",
    "\n",
    "                batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                test_acc.append(batch_acc)\n",
    "            print(\"Train acc: {:.5f}\".format(np.mean(test_acc)))\n",
    "            global_test_acc.append(np.mean(test_acc))\n",
    "        '''    \n",
    "        iteration +=1    \n",
    "    global_train_loss.append(np.mean(temp_train_loss))\n",
    "    \n",
    "    temp_test_loss = []\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {X: x,Y: y,initial_state: test_state,  keep_prob: 1}\n",
    "\n",
    "        batch_acc, test_state, loss_ = sess.run([accuracy, final_state, loss], feed_dict=feed)\n",
    "        temp_test_loss.append(loss_)\n",
    "    \n",
    "    global_test_loss.append(np.mean(temp_test_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Training Completed')\n",
    "print('Total Time Taken: '+str(time.time()-start_time)+' sec' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.77320\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "    feed = {X: x,Y: y,initial_state: test_state, keep_prob: 1}\n",
    "\n",
    "    batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "    test_acc.append(batch_acc)\n",
    "print(\"Train acc: {:.5f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_temp = [92, 282, 1184, 2331, 2815, 2848, 3233, 4833, 7077, 7713, 7837, 8724, 9364, 10162, 10477, 12243, 14445, 15076, 18896, 19741, 19895]\n",
    "for i in wrong_temp[:]:\n",
    "    print('#'+str(i))\n",
    "    print(reviews[i])\n",
    "    print('-->Label='+str(labels[i]))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_l = 0\n",
    "min_l = 10000\n",
    "i = 0\n",
    "m = -1\n",
    "for review in reviews_pp_ints:\n",
    "    if len(review) > max_l:\n",
    "        max_l = len(review)\n",
    "    if len(review) < min_l:\n",
    "        min_l = len(review)\n",
    "        m = i\n",
    "    i += 1\n",
    "reviews[m], m, labels[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_count['terrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_count['horror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [92, 282, 1184, 2331, 2815, 2848, 3233, 4833, 7077, 7713, 7837, 8724, 9364, 10162, 10477, 12243, 14445, 15076, 18896, 19741, 19895]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores of Bad Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for index in a:\n",
    "    state = sess.run(initial_state)\n",
    "\n",
    "    feed = {X: features[index].reshape(1,None) , Y: labels[index].reshape(1,None), initial_state: state}\n",
    "\n",
    "        outputs_ = sess.run([outputs], feed_dict=feed)\n",
    "\n",
    "    print(\"Index:() \".format(index))\n",
    "    print(\"Prediction:{} \".format(outputs_))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Test Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "[0.24922124, 0.25225767, 0.26149547, 0.21638498, 0.24500819, 0.17880425, 0.17892753, 0.17332494, 0.19193126, 0.18435828, 0.21163031, 0.18992403, 0.22238441, 0.23674056, 0.21342954, 0.18122981, 0.1857585, 0.18011074, 0.17807364, 0.18085107, 0.19068667, 0.18928155, 0.1868103, 0.19152905, 0.19835865, 0.19998671, 0.19163567, 0.18537682, 0.18401535, 0.18884964, 0.18799414, 0.18801431, 0.19316795, 0.19291928, 0.19363555, 0.18716188, 0.18127254, 0.18426366, 0.18542771, 0.18857479, 0.19288965, 0.21306169, 0.20296955, 0.19056414, 0.19578031, 0.18438628, 0.19013104, 0.20042448, 0.1848968, 0.19388637, 0.1935045, 0.19468983, 0.19723199, 0.19126642, 0.19377334, 0.1918561, 0.20972607, 0.20698555, 0.20821933, 0.20876786]\n"
     ]
    }
   ],
   "source": [
    "print(len(global_train_loss))\n",
    "print(global_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range( len(global_train_acc) ):\n",
    "    global_train_acc[i] /=100\n",
    "    global_test_acc[i] /=100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27646422, 0.24479373, 0.22284818, 0.18224433, 0.15579242, 0.12269354, 0.09203776, 0.059525914, 0.046631362, 0.041203648, 0.032414231, 0.027642328, 0.022105936, 0.022394991, 0.04220327, 0.023677845, 0.022866067, 0.017994663, 0.012210284, 0.009390913, 0.0083699273, 0.00780555, 0.0079420935, 0.0073836399, 0.008408444, 0.0077077784, 0.0069472282, 0.0067242966, 0.0060860207, 0.0059851394, 0.0071866424, 0.0068035526, 0.0065412819, 0.0061423429, 0.0067196437, 0.0064841295, 0.0063723801, 0.0056546987, 0.0066828071, 0.0064398311, 0.0066907229, 0.0087283561, 0.010279352, 0.0087473588, 0.0079336688, 0.0078925882, 0.0079951929, 0.0077134296, 0.0068428009, 0.0053863726, 0.0058005541, 0.0053988015, 0.0045242789, 0.0055676373, 0.005387472, 0.0051520737, 0.0049751275, 0.0054307943, 0.0062456289, 0.0073672654]\n",
      "[0.24922124, 0.25225767, 0.26149547, 0.21638498, 0.24500819, 0.17880425, 0.17892753, 0.17332494, 0.19193126, 0.18435828, 0.21163031, 0.18992403, 0.22238441, 0.23674056, 0.21342954, 0.18122981, 0.1857585, 0.18011074, 0.17807364, 0.18085107, 0.19068667, 0.18928155, 0.1868103, 0.19152905, 0.19835865, 0.19998671, 0.19163567, 0.18537682, 0.18401535, 0.18884964, 0.18799414, 0.18801431, 0.19316795, 0.19291928, 0.19363555, 0.18716188, 0.18127254, 0.18426366, 0.18542771, 0.18857479, 0.19288965, 0.21306169, 0.20296955, 0.19056414, 0.19578031, 0.18438628, 0.19013104, 0.20042448, 0.1848968, 0.19388637, 0.1935045, 0.19468983, 0.19723199, 0.19126642, 0.19377334, 0.1918561, 0.20972607, 0.20698555, 0.20821933, 0.20876786]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncur/A0zJTN/PTW0SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ca91c7518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Testing Loss: 0.173325\n"
     ]
    }
   ],
   "source": [
    "print(global_train_loss)\n",
    "print(global_test_loss)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.ion()\n",
    "x = range(60) \n",
    "plt.axis([0,70,0,0.3])\n",
    "plt.plot(x,global_train_loss,'go',x,global_test_loss,'ro')\n",
    "plt.show()\n",
    "\n",
    "print('Minimum Testing Loss: '+str(np.min(global_test_loss)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(reviews[92])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
