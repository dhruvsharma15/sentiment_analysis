{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MILESTONE 2\n",
    "\n",
    "IMDB dataset + Siraj's Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset\n",
    "\n",
    "1. Removing punctuations\n",
    "2. Generating word_to_int map\n",
    "3. Coverting each review in ints\n",
    "4. Padding each review with 0's and generating input of length 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <PERIOD> ')\n",
    "    text = text.replace('\"', ' <PERIOD> ')\n",
    "    text = text.replace(';', ' <PERIOD> ')\n",
    "    text = text.replace('!', ' <PERIOD> ')\n",
    "    text = text.replace('?', ' <PERIOD> ')\n",
    "    text = text.replace('(', ' <PERIOD> ')\n",
    "    text = text.replace(')', ' <PERIOD> ')\n",
    "    text = text.replace('--', ' <PERIOD> ')\n",
    "    text = text.replace('?', ' <PERIOD> ')\n",
    "    '''\n",
    "    text = text.replace('<br />', ' <PERIOD> ')\n",
    "    text = text.replace('\\\\', ' <PERIOD> ')\n",
    "    text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <PERIOD> ')\n",
    "    text = text.replace(' <PERIOD> ', ' ')\n",
    "    words = text.split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "def removing_noise(words):\n",
    "    word_count = Counter(words)\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    words_new = [word for word in words if (word_count[word]>5) #and (not word in stops)\n",
    "                ]\n",
    "    return words_new\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'data/labeledTrainData.tsv'\n",
    "review_ids = []\n",
    "reviews = []\n",
    "labels = []\n",
    "#importing dataset into lists\n",
    "wrong_temp = []\n",
    "#[92, 102, 120, 259, 404, 1028, 1094, 1184, 1229, 1234, 1343, 1503, 1790, 1861, 2212, 3430, 3771, 3870, 4106, 4407, 4866, 5053, 5218, 5221, 5514, 5553, 5646, 5853, 6086, 6499, 6582, 6746, 7021, 7023, 7194, 7331, 7454, 7473, 7553, 7837, 8119, 8264, 8407, 8433, 8971, 9076, 9204, 9402, 9490, 9552, 9562, 9632, 9716, 9748, 9787, 10107, 10230, 10233, 10414, 10477, 10500, 10702, 10892, 11048, 11055, 11371, 11375, 11513, 11744, 11944, 12071, 12159, 12188, 12243, 12341, 12558, 12594, 12808, 13087, 13159, 14111, 14755, 14860, 14993, 15094, 15260, 15352, 15360, 15656, 15871, 16214, 16274, 16492, 16539, 16613, 16622, 16870, 16949, 16990, 16992, 17071, 17254, 17371, 17450, 17463, 17603, 17709, 17712, 17749, 18024, 18221, 18226, 18681, 18784, 18896, 19063, 19609, 19714, 19889]\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    row_count = 0\n",
    "    for row in reader:\n",
    "        review_ids.append(row[0])\n",
    "        labels.append([int(row[1])] )\n",
    "        reviews.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_pp = []\n",
    "words = []\n",
    "\n",
    "for review in reviews:\n",
    "    review_pp = preprocess(review)\n",
    "    reviews_pp.append(review_pp)\n",
    "    words.extend(review_pp)\n",
    "    \n",
    "words = removing_noise(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting word to integers and making the vocabulary\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "words_count = Counter(words)\n",
    "sorted_vocab = sorted(words_count, key = words_count.get, reverse = True)\n",
    "word_to_int = {word:i for i,word in enumerate(sorted_vocab,1)}\n",
    "\n",
    "#Converting each review in the form of integers\n",
    "reviews_pp_ints = []\n",
    "for review in reviews_pp:\n",
    "    this_review_int = []\n",
    "    for word in review:\n",
    "        if word in vocab:\n",
    "            this_review_int.append(word_to_int[word])\n",
    "    reviews_pp_ints.append(this_review_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp_ints[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_len = 200\n",
    "features = np.zeros((len(reviews_pp_ints), max_seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_pp_ints):\n",
    "    features[i, :len(row)] = np.array(row[:max_seq_len] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'features' is a 2d array storing all sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "embed_size = 100\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "hidden_nodes = 10\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'labels')\n",
    "#keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "tf.set_random_seed(5)\n",
    "\n",
    "embedding = tf.Variable(tf.random_uniform((vocab_size+1, embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "#drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm]*lstm_layers)\n",
    "\n",
    "#getting an initial state of zeros\\n\",\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state = initial_state)\n",
    "\n",
    "#hidden_layer = tf.contrib.layers.fully_connected(outputs[:, -1], hidden_nodes, activation_fn=tf.nn.relu)\n",
    "predictions = tf.contrib.layers.fully_connected(outputs[:, -1],1, activation_fn=tf.sigmoid)\n",
    "#predictions,Y\\n\",\n",
    "\n",
    "#regularizers =  tf.reduce_mean(tf.nn.l2_loss(tf.trainable_variables() ))\n",
    "regularizers =sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name))\n",
    "loss = tf.reduce_mean(tf.square(Y - predictions) )+ 0.01*regularizers\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy:\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.float32), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/60 Iteration: 5 Train loss: 6167.59180\n",
      "Epoch: 0/60 Iteration: 10 Train loss: 5261.15039\n",
      "Epoch: 0/60 Iteration: 15 Train loss: 4459.77881\n",
      "Epoch: 0/60 Iteration: 20 Train loss: 3758.37744\n",
      "Epoch: 0/60 Iteration: 25 Train loss: 3150.28760\n",
      "Epoch: 0/60 Iteration: 30 Train loss: 2627.18066\n",
      "Epoch: 0/60 Iteration: 35 Train loss: 2180.37549\n",
      "Epoch: 0/60 Iteration: 40 Train loss: 1801.20459\n",
      "Epoch: 1/60 Iteration: 5 Train loss: 1481.27014\n",
      "Epoch: 1/60 Iteration: 10 Train loss: 1212.75098\n",
      "Epoch: 1/60 Iteration: 15 Train loss: 988.51337\n",
      "Epoch: 1/60 Iteration: 20 Train loss: 802.14594\n",
      "Epoch: 1/60 Iteration: 25 Train loss: 647.98145\n",
      "Epoch: 1/60 Iteration: 30 Train loss: 521.04834\n",
      "Epoch: 1/60 Iteration: 35 Train loss: 417.02432\n",
      "Epoch: 1/60 Iteration: 40 Train loss: 332.17886\n",
      "Epoch: 2/60 Iteration: 5 Train loss: 263.31058\n",
      "Epoch: 2/60 Iteration: 10 Train loss: 207.68646\n",
      "Epoch: 2/60 Iteration: 15 Train loss: 162.98671\n",
      "Epoch: 2/60 Iteration: 20 Train loss: 127.25143\n",
      "Epoch: 2/60 Iteration: 25 Train loss: 98.83425\n",
      "Epoch: 2/60 Iteration: 30 Train loss: 76.35890\n",
      "Epoch: 2/60 Iteration: 35 Train loss: 58.68132\n",
      "Epoch: 2/60 Iteration: 40 Train loss: 44.85589\n",
      "Epoch: 3/60 Iteration: 5 Train loss: 34.10538\n",
      "Epoch: 3/60 Iteration: 10 Train loss: 25.79529\n",
      "Epoch: 3/60 Iteration: 15 Train loss: 19.40973\n",
      "Epoch: 3/60 Iteration: 20 Train loss: 14.53302\n",
      "Epoch: 3/60 Iteration: 25 Train loss: 10.83163\n",
      "Epoch: 3/60 Iteration: 30 Train loss: 8.04006\n",
      "Epoch: 3/60 Iteration: 35 Train loss: 5.94793\n",
      "Epoch: 3/60 Iteration: 40 Train loss: 4.39044\n",
      "Epoch: 4/60 Iteration: 5 Train loss: 3.23845\n",
      "Epoch: 4/60 Iteration: 10 Train loss: 2.39242\n",
      "Epoch: 4/60 Iteration: 15 Train loss: 1.77510\n",
      "Epoch: 4/60 Iteration: 20 Train loss: 1.32799\n",
      "Epoch: 4/60 Iteration: 25 Train loss: 1.00640\n",
      "Epoch: 4/60 Iteration: 30 Train loss: 0.77687\n",
      "Epoch: 4/60 Iteration: 35 Train loss: 0.61400\n",
      "Epoch: 4/60 Iteration: 40 Train loss: 0.49958\n",
      "Epoch: 5/60 Iteration: 5 Train loss: 0.41962\n",
      "Epoch: 5/60 Iteration: 10 Train loss: 0.36446\n",
      "Epoch: 5/60 Iteration: 15 Train loss: 0.32649\n",
      "Epoch: 5/60 Iteration: 20 Train loss: 0.30072\n",
      "Epoch: 5/60 Iteration: 25 Train loss: 0.28334\n",
      "Epoch: 5/60 Iteration: 30 Train loss: 0.27181\n",
      "Epoch: 5/60 Iteration: 35 Train loss: 0.26394\n",
      "Epoch: 5/60 Iteration: 40 Train loss: 0.25890\n",
      "Epoch: 6/60 Iteration: 5 Train loss: 0.25550\n",
      "Epoch: 6/60 Iteration: 10 Train loss: 0.25350\n",
      "Epoch: 6/60 Iteration: 15 Train loss: 0.25212\n",
      "Epoch: 6/60 Iteration: 20 Train loss: 0.25134\n",
      "Epoch: 6/60 Iteration: 25 Train loss: 0.25085\n",
      "Epoch: 6/60 Iteration: 30 Train loss: 0.25065\n",
      "Epoch: 6/60 Iteration: 35 Train loss: 0.25029\n",
      "Epoch: 6/60 Iteration: 40 Train loss: 0.25019\n",
      "Epoch: 7/60 Iteration: 5 Train loss: 0.25000\n",
      "Epoch: 7/60 Iteration: 10 Train loss: 0.25008\n",
      "Epoch: 7/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 7/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 7/60 Iteration: 25 Train loss: 0.25008\n",
      "Epoch: 7/60 Iteration: 30 Train loss: 0.25020\n",
      "Epoch: 7/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 7/60 Iteration: 40 Train loss: 0.25004\n",
      "Epoch: 8/60 Iteration: 5 Train loss: 0.24992\n",
      "Epoch: 8/60 Iteration: 10 Train loss: 0.25003\n",
      "Epoch: 8/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 8/60 Iteration: 20 Train loss: 0.25005\n",
      "Epoch: 8/60 Iteration: 25 Train loss: 0.25008\n",
      "Epoch: 8/60 Iteration: 30 Train loss: 0.25020\n",
      "Epoch: 8/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 8/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 9/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 9/60 Iteration: 10 Train loss: 0.25003\n",
      "Epoch: 9/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 9/60 Iteration: 20 Train loss: 0.25005\n",
      "Epoch: 9/60 Iteration: 25 Train loss: 0.25008\n",
      "Epoch: 9/60 Iteration: 30 Train loss: 0.25021\n",
      "Epoch: 9/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 9/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 10/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 10/60 Iteration: 10 Train loss: 0.25003\n",
      "Epoch: 10/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 10/60 Iteration: 20 Train loss: 0.25005\n",
      "Epoch: 10/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 10/60 Iteration: 30 Train loss: 0.25021\n",
      "Epoch: 10/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 10/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 11/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 11/60 Iteration: 10 Train loss: 0.25003\n",
      "Epoch: 11/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 11/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 11/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 11/60 Iteration: 30 Train loss: 0.25022\n",
      "Epoch: 11/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 11/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 12/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 12/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 12/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 12/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 12/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 12/60 Iteration: 30 Train loss: 0.25022\n",
      "Epoch: 12/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 12/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 13/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 13/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 13/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 13/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 13/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 13/60 Iteration: 30 Train loss: 0.25022\n",
      "Epoch: 13/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 13/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 14/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 14/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 14/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 14/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 14/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 14/60 Iteration: 30 Train loss: 0.25022\n",
      "Epoch: 14/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 14/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 15/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 15/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 15/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 15/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 15/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 15/60 Iteration: 30 Train loss: 0.25022\n",
      "Epoch: 15/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 15/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 16/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 16/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 16/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 16/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 16/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 16/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 16/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 16/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 17/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 17/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 17/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 17/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 17/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 17/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 17/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 17/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 18/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 18/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 18/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 18/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 18/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 18/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 18/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 18/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 19/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 19/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 19/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 19/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 19/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 19/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 19/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 19/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 20/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 20/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 20/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 20/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 20/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 20/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 20/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 20/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 21/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 21/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 21/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 21/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 21/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 21/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 21/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 21/60 Iteration: 40 Train loss: 0.25005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 22/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 22/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 22/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 22/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 22/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 22/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 22/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 23/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 23/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 23/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 23/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 23/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 23/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 23/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 23/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 24/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 24/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 24/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 24/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 24/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 24/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 24/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 24/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 25/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 25/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 25/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 25/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 25/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 25/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 25/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 25/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 26/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 26/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 26/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 26/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 26/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 26/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 26/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 26/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 27/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 27/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 27/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 27/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 27/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 27/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 27/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 27/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 28/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 28/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 28/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 28/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 28/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 28/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 28/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 28/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 29/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 29/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 29/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 29/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 29/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 29/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 29/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 29/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 30/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 30/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 30/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 30/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 30/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 30/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 30/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 30/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 31/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 31/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 31/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 31/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 31/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 31/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 31/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 31/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 32/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 32/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 32/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 32/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 32/60 Iteration: 25 Train loss: 0.25011\n",
      "Epoch: 32/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 32/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 32/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 33/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 33/60 Iteration: 10 Train loss: 0.25011\n",
      "Epoch: 33/60 Iteration: 15 Train loss: 0.25006\n",
      "Epoch: 33/60 Iteration: 20 Train loss: 0.25013\n",
      "Epoch: 33/60 Iteration: 25 Train loss: 0.25016\n",
      "Epoch: 33/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 33/60 Iteration: 35 Train loss: 0.25008\n",
      "Epoch: 33/60 Iteration: 40 Train loss: 0.25010\n",
      "Epoch: 34/60 Iteration: 5 Train loss: 0.24996\n",
      "Epoch: 34/60 Iteration: 10 Train loss: 0.25008\n",
      "Epoch: 34/60 Iteration: 15 Train loss: 0.25003\n",
      "Epoch: 34/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 34/60 Iteration: 25 Train loss: 0.25014\n",
      "Epoch: 34/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 34/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 34/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 35/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 35/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 35/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 35/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 35/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 35/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 35/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 35/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 36/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 36/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 36/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 36/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 36/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 36/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 36/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 36/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 37/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 37/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 37/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 37/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 37/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 37/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 37/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 37/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 38/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 38/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 38/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 38/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 38/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 38/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 38/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 38/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 39/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 39/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 39/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 39/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 39/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 39/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 39/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 39/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 40/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 40/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 40/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 40/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 40/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 40/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 40/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 40/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 41/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 41/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 41/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 41/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 41/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 41/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 41/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 41/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 42/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 42/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 42/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 42/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 42/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 42/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 42/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 42/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 43/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 43/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 43/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 43/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 43/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 43/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 43/60 Iteration: 35 Train loss: 0.25006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 44/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 44/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 44/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 44/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 44/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 44/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 44/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 44/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 45/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 45/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 45/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 45/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 45/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 45/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 45/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 45/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 46/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 46/60 Iteration: 10 Train loss: 0.25008\n",
      "Epoch: 46/60 Iteration: 15 Train loss: 0.25003\n",
      "Epoch: 46/60 Iteration: 20 Train loss: 0.25011\n",
      "Epoch: 46/60 Iteration: 25 Train loss: 0.25015\n",
      "Epoch: 46/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 46/60 Iteration: 35 Train loss: 0.25008\n",
      "Epoch: 46/60 Iteration: 40 Train loss: 0.25011\n",
      "Epoch: 47/60 Iteration: 5 Train loss: 0.24996\n",
      "Epoch: 47/60 Iteration: 10 Train loss: 0.25009\n",
      "Epoch: 47/60 Iteration: 15 Train loss: 0.25004\n",
      "Epoch: 47/60 Iteration: 20 Train loss: 0.25012\n",
      "Epoch: 47/60 Iteration: 25 Train loss: 0.25015\n",
      "Epoch: 47/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 47/60 Iteration: 35 Train loss: 0.25008\n",
      "Epoch: 47/60 Iteration: 40 Train loss: 0.25011\n",
      "Epoch: 48/60 Iteration: 5 Train loss: 0.24996\n",
      "Epoch: 48/60 Iteration: 10 Train loss: 0.25010\n",
      "Epoch: 48/60 Iteration: 15 Train loss: 0.25004\n",
      "Epoch: 48/60 Iteration: 20 Train loss: 0.25013\n",
      "Epoch: 48/60 Iteration: 25 Train loss: 0.25016\n",
      "Epoch: 48/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 48/60 Iteration: 35 Train loss: 0.25008\n",
      "Epoch: 48/60 Iteration: 40 Train loss: 0.25010\n",
      "Epoch: 49/60 Iteration: 5 Train loss: 0.24995\n",
      "Epoch: 49/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 49/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 49/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 49/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 49/60 Iteration: 30 Train loss: 0.25026\n",
      "Epoch: 49/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 49/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 50/60 Iteration: 5 Train loss: 0.24993\n",
      "Epoch: 50/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 50/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 50/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 50/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 50/60 Iteration: 30 Train loss: 0.25026\n",
      "Epoch: 50/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 50/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 51/60 Iteration: 5 Train loss: 0.24993\n",
      "Epoch: 51/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 51/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 51/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 51/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 51/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 51/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 51/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 52/60 Iteration: 5 Train loss: 0.24993\n",
      "Epoch: 52/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 52/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 52/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 52/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 52/60 Iteration: 30 Train loss: 0.25026\n",
      "Epoch: 52/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 52/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 53/60 Iteration: 5 Train loss: 0.24993\n",
      "Epoch: 53/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 53/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 53/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 53/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 53/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 53/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 53/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 54/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 54/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 54/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 54/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 54/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 54/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 54/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 54/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 55/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 55/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 55/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 55/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 55/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 55/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 55/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 55/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 56/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 56/60 Iteration: 10 Train loss: 0.25009\n",
      "Epoch: 56/60 Iteration: 15 Train loss: 0.25004\n",
      "Epoch: 56/60 Iteration: 20 Train loss: 0.25013\n",
      "Epoch: 56/60 Iteration: 25 Train loss: 0.25016\n",
      "Epoch: 56/60 Iteration: 30 Train loss: 0.25030\n",
      "Epoch: 56/60 Iteration: 35 Train loss: 0.25008\n",
      "Epoch: 56/60 Iteration: 40 Train loss: 0.25011\n",
      "Epoch: 57/60 Iteration: 5 Train loss: 0.24996\n",
      "Epoch: 57/60 Iteration: 10 Train loss: 0.25009\n",
      "Epoch: 57/60 Iteration: 15 Train loss: 0.25004\n",
      "Epoch: 57/60 Iteration: 20 Train loss: 0.25012\n",
      "Epoch: 57/60 Iteration: 25 Train loss: 0.25015\n",
      "Epoch: 57/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 57/60 Iteration: 35 Train loss: 0.25007\n",
      "Epoch: 57/60 Iteration: 40 Train loss: 0.25010\n",
      "Epoch: 58/60 Iteration: 5 Train loss: 0.24996\n",
      "Epoch: 58/60 Iteration: 10 Train loss: 0.25009\n",
      "Epoch: 58/60 Iteration: 15 Train loss: 0.25004\n",
      "Epoch: 58/60 Iteration: 20 Train loss: 0.25012\n",
      "Epoch: 58/60 Iteration: 25 Train loss: 0.25015\n",
      "Epoch: 58/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 58/60 Iteration: 35 Train loss: 0.25007\n",
      "Epoch: 58/60 Iteration: 40 Train loss: 0.25011\n",
      "Epoch: 59/60 Iteration: 5 Train loss: 0.24996\n",
      "Epoch: 59/60 Iteration: 10 Train loss: 0.25009\n",
      "Epoch: 59/60 Iteration: 15 Train loss: 0.25004\n",
      "Epoch: 59/60 Iteration: 20 Train loss: 0.25012\n",
      "Epoch: 59/60 Iteration: 25 Train loss: 0.25015\n",
      "Epoch: 59/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 59/60 Iteration: 35 Train loss: 0.25007\n",
      "Epoch: 59/60 Iteration: 40 Train loss: 0.25010\n",
      "Training Completed\n",
      "Total Time Taken: 826.5151345729828 sec\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "global_train_acc = []\n",
    "global_test_acc = []\n",
    "\n",
    "global_train_loss = []\n",
    "global_test_loss = []\n",
    "import time\n",
    "start_time = time.time()\n",
    "for e in range(n_epochs):\n",
    "    state = sess.run(initial_state)\n",
    "    iteration = 1\n",
    "    loss_=0.0\n",
    "    temp_train_loss = []\n",
    "    for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "        feed = {X: x, Y: y, initial_state: state}#, keep_prob: 0.5 }\n",
    "\n",
    "        state, loss_,  _ = sess.run([final_state, loss, optimizer], feed_dict=feed)\n",
    "\n",
    "        if iteration%5==0:\n",
    "            print(\"Epoch: {}/{}\".format(e, n_epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Train loss: {:.5f}\".format(loss_))\n",
    "        temp_train_loss.append(loss_)\n",
    "        '''\n",
    "        if iteration%25==0:\n",
    "            val_acc = []\n",
    "            val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                feed = {X: x,\n",
    "                        Y: y,\n",
    "                        initial_state: val_state}\n",
    "                batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                val_acc.append(batch_acc)\n",
    "            print(\"Val acc: {:.5f}\".format(np.mean(val_acc)))\n",
    "        '''\n",
    "        '''\n",
    "        if iteration%25==0:\n",
    "            # train Acc calculation\n",
    "            train_acc = []\n",
    "            train_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for x, y in get_batches(train_x, train_y, batch_size):\n",
    "                feed = {X: x,\n",
    "                        Y: y,\n",
    "                        initial_state: train_state}\n",
    "                batch_acc, train_state, corr = sess.run([accuracy, final_state, correct_pred], feed_dict=feed)\n",
    "                bad_indexes = [index for index, correctness in enumerate(corr) if correctness ==0 ]\n",
    "                train_acc.append(batch_acc)\n",
    "            print(\"Train acc: {:.5f}\".format(np.mean(train_acc)))\n",
    "            global_train_acc.append(np.mean(train_acc))\n",
    "            \n",
    "            # test acc calculation\n",
    "            test_acc = []\n",
    "            test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "                feed = {X: x,Y: y,initial_state: test_state}\n",
    "\n",
    "                batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                test_acc.append(batch_acc)\n",
    "            print(\"Train acc: {:.5f}\".format(np.mean(test_acc)))\n",
    "            global_test_acc.append(np.mean(test_acc))\n",
    "        '''    \n",
    "        iteration +=1    \n",
    "    global_train_loss.append(np.mean(temp_train_loss))\n",
    "    \n",
    "    temp_test_loss = []\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {X: x,Y: y,initial_state: test_state} # , keep_prob: 1 }\n",
    "\n",
    "        batch_acc, test_state, loss_ = sess.run([accuracy, final_state, loss], feed_dict=feed)\n",
    "        temp_test_loss.append(loss_)\n",
    "    \n",
    "    global_test_loss.append(np.mean(temp_test_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Training Completed')\n",
    "print('Total Time Taken: '+str(time.time()-start_time)+' sec' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.49840\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "    feed = {X: x,Y: y,initial_state: test_state}#, keep_prob: 1}\n",
    "\n",
    "    batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "    test_acc.append(batch_acc)\n",
    "print(\"Train acc: {:.5f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "wrong_temp = [92, 282, 1184, 2331, 2815, 2848, 3233, 4833, 7077, 7713, 7837, 8724, 9364, 10162, 10477, 12243, 14445, 15076, 18896, 19741, 19895]\n",
    "for i in wrong_temp[:]:\n",
    "    print('#'+str(i))\n",
    "    print(reviews[i])\n",
    "    print('-->Label='+str(labels[i]))\n",
    "    print()\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_l = 0\n",
    "min_l = 10000\n",
    "i = 0\n",
    "m = -1\n",
    "for review in reviews_pp_ints:\n",
    "\n",
    "    if len(review) > max_l:\n",
    "        max_l = len(review)\n",
    "    if len(review) < min_l:\n",
    "        min_l = len(review)\n",
    "        m = i\n",
    "    i += 1\n",
    "reviews[m], m, labels[m]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count['terrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count['horror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [92, 282, 1184, 2331, 2815, 2848, 3233, 4833, 7077, 7713, 7837, 8724, 9364, 10162, 10477, 12243, 14445, 15076, 18896, 19741, 19895]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores of Bad Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for index in a:\n",
    "    state = sess.run(initial_state)\n",
    "\n",
    "    feed = {X: features[index].reshape(1,None) , Y: labels[index].reshape(1,None), initial_state: state}\n",
    "\n",
    "        outputs_ = sess.run([outputs], feed_dict=feed)\n",
    "\n",
    "    print(\"Index:() \".format(index))\n",
    "    print(\"Prediction:{} \".format(outputs_))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Test Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(global_train_loss))\n",
    "print(global_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range( len(global_train_acc) ):\n",
    "    global_train_acc[i] /=100\n",
    "    global_test_acc[i] /=100\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3937.6453, 871.09827, 143.72031, 17.296738, 1.6360394, 0.32231849, 0.25223666, 0.25010592, 0.25007883, 0.25008047, 0.25008193, 0.25008318, 0.25008425, 0.25008518, 0.25008601, 0.25008669, 0.25008732, 0.25008792, 0.25008839, 0.2500889, 0.25008938, 0.2500897, 0.25009006, 0.2500906, 0.25009099, 0.25009128, 0.25009155, 0.25009185, 0.25009236, 0.25009292, 0.25009325, 0.25009426, 0.2500959, 0.2501463, 0.25012937, 0.25012317, 0.25012082, 0.25012076, 0.25012073, 0.25011986, 0.25011951, 0.25011909, 0.25012055, 0.2501213, 0.25012112, 0.25012264, 0.25013608, 0.25014266, 0.25014639, 0.25011954, 0.25011331, 0.25011489, 0.25011539, 0.25011721, 0.25011835, 0.25012225, 0.25014442, 0.25014275, 0.2501412, 0.25014085]\n",
      "[1732.7402, 317.22092, 42.482044, 4.1311865, 0.481215, 0.25812745, 0.25017908, 0.25004664, 0.25004756, 0.25004897, 0.25005007, 0.25005114, 0.25005195, 0.25005275, 0.25005347, 0.25005406, 0.25005466, 0.25005516, 0.25005555, 0.25005603, 0.25005645, 0.25005671, 0.25005713, 0.2500577, 0.25005797, 0.25005817, 0.25005871, 0.25005907, 0.25005949, 0.25005999, 0.25006056, 0.25006136, 0.25006574, 0.25010651, 0.25009304, 0.25008911, 0.25008625, 0.25008756, 0.25008863, 0.25008592, 0.25008714, 0.2500869, 0.25008839, 0.25008732, 0.25008851, 0.25009137, 0.25011259, 0.25011164, 0.25010121, 0.25007978, 0.25008103, 0.2500841, 0.25008357, 0.25008544, 0.25008789, 0.25008851, 0.25011498, 0.25010857, 0.25010854, 0.25010723]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSWqUASBJjTIAJKlRBoAkNer/A8bfZnryeMVOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f43671e0d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Testing Loss: 0.250047\n"
     ]
    }
   ],
   "source": [
    "print(global_train_loss)\n",
    "print(global_test_loss)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.ion()\n",
    "x = range(60) \n",
    "plt.axis([0,70,0,0.3])\n",
    "plt.plot(x,global_train_loss,'go',x,global_test_loss,'ro')\n",
    "plt.show()\n",
    "\n",
    "print('Minimum Testing Loss: '+str(np.min(global_test_loss)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews[92])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
