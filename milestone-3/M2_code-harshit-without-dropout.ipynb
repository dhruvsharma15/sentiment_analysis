{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MILESTONE 2\n",
    "\n",
    "IMDB dataset + Siraj's Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset\n",
    "\n",
    "1. Removing punctuations\n",
    "2. Generating word_to_int map\n",
    "3. Coverting each review in ints\n",
    "4. Padding each review with 0's and generating input of length 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    \n",
    "    text = text.lower()\n",
    "#     text = text.replace('.', ' <PERIOD> ')\n",
    "#     text = text.replace(',', ' <PERIOD> ')\n",
    "#     text = text.replace('\"', ' <PERIOD> ')\n",
    "#     text = text.replace(';', ' <PERIOD> ')\n",
    "#     text = text.replace('!', ' <PERIOD> ')\n",
    "#     text = text.replace('?', ' <PERIOD> ')\n",
    "#     text = text.replace('(', ' <PERIOD> ')\n",
    "#     text = text.replace(')', ' <PERIOD> ')\n",
    "#     text = text.replace('--', ' <PERIOD> ')\n",
    "#     text = text.replace('?', ' <PERIOD> ')\n",
    "    \n",
    "    text = text.replace('<br />', ' <PERIOD> ')\n",
    "    text = text.replace('\\\\', ' <PERIOD> ')\n",
    "    text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <PERIOD> ')\n",
    "    text = text.replace(' <PERIOD> ', ' ')\n",
    "    words = text.split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "def removing_noise(words):\n",
    "    word_count = Counter(words)\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    words_new = [word for word in words if (word_count[word]>5) #and (not word in stops)\n",
    "                ]\n",
    "    return words_new\n",
    "    \n",
    "\n",
    "sentence = []\n",
    "score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_ids = []\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "import csv\n",
    "\n",
    "filename = 'labeledTrainData.tsv'\n",
    "\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    row_count = 0\n",
    "    for row in reader:\n",
    "        review_ids.append(row[0])\n",
    "        labels.append(int(row[1]) )\n",
    "        reviews.append(row[2])\n",
    "\n",
    "sentence.extend(reviews)\n",
    "score.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('droom_google_tagged_data.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "    \n",
    "\n",
    "sentiments = []\n",
    "dialogues = []\n",
    "for d in data['data']:\n",
    "    #d['conversation']\n",
    "    #d['conversation_id']\n",
    "    conversation = d['conversation']\n",
    "    c_id = d['conversation_id']\n",
    "    for c in conversation:\n",
    "        #c['sentiment']\n",
    "        #c['text']\n",
    "        #c['side']\n",
    "        #c['timestamp']\n",
    "        sentiment = float(c['sentiment'])\n",
    "        sentiment = -1 if (sentiment < 0) else 1\n",
    "        dialogue = c['text']\n",
    "        dialogues.append(dialogue)\n",
    "        sentiments.append(sentiment)\n",
    "\n",
    "sentence.extend(dialogues)\n",
    "score.extend(sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_pp = []\n",
    "words = []\n",
    "\n",
    "for s in sentence:\n",
    "    review_pp = preprocess(s)\n",
    "    sentence_pp.append(review_pp)\n",
    "    words.extend(review_pp)\n",
    "    \n",
    "words = removing_noise(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting word to integers and making the vocabulary\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "words_count = Counter(words)\n",
    "sorted_vocab = sorted(words_count, key = words_count.get, reverse = True)\n",
    "word_to_int = {word:i for i,word in enumerate(sorted_vocab,1)}\n",
    "\n",
    "#Converting each review in the form of integers\n",
    "sentence_pp_ints = []\n",
    "for review in sentence_pp:\n",
    "    this_review_int = []\n",
    "    for word in review:\n",
    "        if word in vocab:\n",
    "            this_review_int.append(word_to_int[word])\n",
    "    sentence_pp_ints.append(this_review_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from random import shuffle\n",
    "# l = list(zip(reviews_pp_ints, labels))\n",
    "# shuffle(l)\n",
    "\n",
    "# reviews_pp_ints = [ item[0] for item in l]\n",
    "# labels = [ [item[1]] for item in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_len = 200\n",
    "features = np.zeros((len(sentence_pp_ints), max_seq_len), dtype=int)\n",
    "for i, row in enumerate(sentence_pp_ints):\n",
    "    features[i, :len(row)] = np.array(row[:max_seq_len] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = features[:len(reviews)]\n",
    "dialogues = features[len(reviews):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_frac = 0.6\n",
    "split_idx = int(len(dialogues)*split_frac)\n",
    "\n",
    "dialogues_train, dialogues_val = dialogues[:split_idx], dialogues[split_idx:]\n",
    "sentiments_train, sentiments_val = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(dialogues_val)*0.5)\n",
    "dialogues_val, dialogues_test = dialogues_val[:test_idx], dialogues_val[test_idx:]\n",
    "sentiments_val, sentiments_test = sentiments_val[:test_idx], sentiments_val[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_frac = 0.6\n",
    "split_idx = int(len(reviews)*split_frac)\n",
    "\n",
    "reviews_train, reviews_val = reviews[:split_idx], reviews[split_idx:]\n",
    "labels_train, labels_val = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(reviews_val)*0.5)\n",
    "reviews_val, reviews_test = reviews_val[:test_idx], reviews_val[test_idx:]\n",
    "labels_val, labels_test = labels_val[:test_idx], labels_val[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split_frac = 0.8\n",
    "# split_idx = int(len(features)*0.8)\n",
    "# train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "# train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# test_idx = int(len(val_x)*0.5)\n",
    "# val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "# val_y, test_y = val_y[:test_idx], val_y[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = []\n",
    "val_x = []\n",
    "train_y = []\n",
    "val_y = []\n",
    "\n",
    "train_x.extend(reviews_train)\n",
    "train_x.extend(dialogues_train)\n",
    "\n",
    "# val_x.extend(reviews_val)\n",
    "# val_x.extend(dialogues_val)\n",
    "\n",
    "train_y.extend(labels_train)\n",
    "train_y.extend(sentiments_train)\n",
    "\n",
    "# val_y.extend(labels_val)\n",
    "# val_y.extend(sentiments_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "train = list(zip(train_x, train_y))\n",
    "shuffle(train)\n",
    "\n",
    "train_x = [ item[0] for item in train]\n",
    "train_y = [ [item[1]] for item in train]\n",
    "\n",
    "labels_val = [ [item] for item in labels_val]\n",
    "labels_test = [ [item] for item in labels_test]\n",
    "\n",
    "sentiments_val = [ [item] for item in sentiments_val]\n",
    "sentiments_test = [ [item] for item in sentiments_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from random import shuffle\n",
    "# val = list(zip(val_x, val_y))\n",
    "# shuffle(val)\n",
    "\n",
    "# val_x = [ item[0] for item in val]\n",
    "# val_y = [ [item[1]] for item in val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "embed_size = 100\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "hidden_nodes = 10\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'labels')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "tf.set_random_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'Embedding_models/glove.6B.300d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# embedding = tf.Variable(tf.random_uniform((vocab_size+1, embed_size), -1, 1))\n",
    "# embed = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "\n",
    "\n",
    "############# NEW GLOVE ################\n",
    "\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)\n",
    "\n",
    "#######################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm]*lstm_layers)\n",
    "\n",
    "#getting an initial state of zeros\\n\",\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state = initial_state)\n",
    "\n",
    "#hidden_layer = tf.contrib.layers.fully_connected(outputs[:, -1], hidden_nodes, activation_fn=tf.nn.relu)\n",
    "predictions = tf.contrib.layers.fully_connected(outputs[:, -1],1, activation_fn=tf.sigmoid)\n",
    "#predictions,Y\\n\",\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(Y - predictions))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy:\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.float32), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/30 Iteration: 5 Train loss: 0.38035\n",
      "Epoch: 0/30 Iteration: 10 Train loss: 0.29193\n",
      "Epoch: 0/30 Iteration: 15 Train loss: 0.32201\n",
      "Epoch: 0/30 Iteration: 20 Train loss: 0.30021\n",
      "Epoch: 0/30 Iteration: 25 Train loss: 0.30190\n",
      "Epoch: 0/30 Iteration: 30 Train loss: 0.30530\n",
      "Epoch: 1/30 Iteration: 5 Train loss: 0.30568\n",
      "Epoch: 1/30 Iteration: 10 Train loss: 0.27005\n",
      "Epoch: 1/30 Iteration: 15 Train loss: 0.31691\n",
      "Epoch: 1/30 Iteration: 20 Train loss: 0.29912\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.29873\n",
      "Epoch: 1/30 Iteration: 30 Train loss: 0.30028\n",
      "Epoch: 2/30 Iteration: 5 Train loss: 0.29874\n",
      "Epoch: 2/30 Iteration: 10 Train loss: 0.25833\n",
      "Epoch: 2/30 Iteration: 15 Train loss: 0.30011\n",
      "Epoch: 2/30 Iteration: 20 Train loss: 0.27551\n",
      "Epoch: 2/30 Iteration: 25 Train loss: 0.27786\n",
      "Epoch: 2/30 Iteration: 30 Train loss: 0.28085\n",
      "Epoch: 3/30 Iteration: 5 Train loss: 0.27670\n",
      "Epoch: 3/30 Iteration: 10 Train loss: 0.23460\n",
      "Epoch: 3/30 Iteration: 15 Train loss: 0.26524\n",
      "Epoch: 3/30 Iteration: 20 Train loss: 0.23628\n",
      "Epoch: 3/30 Iteration: 25 Train loss: 0.25092\n",
      "Epoch: 3/30 Iteration: 30 Train loss: 0.25512\n",
      "Epoch: 4/30 Iteration: 5 Train loss: 0.24795\n",
      "Epoch: 4/30 Iteration: 10 Train loss: 0.20906\n",
      "Epoch: 4/30 Iteration: 15 Train loss: 0.24682\n",
      "Epoch: 4/30 Iteration: 20 Train loss: 0.22688\n",
      "Epoch: 4/30 Iteration: 25 Train loss: 0.23485\n",
      "Epoch: 4/30 Iteration: 30 Train loss: 0.23503\n",
      "Epoch: 5/30 Iteration: 5 Train loss: 0.21490\n",
      "Epoch: 5/30 Iteration: 10 Train loss: 0.16979\n",
      "Epoch: 5/30 Iteration: 15 Train loss: 0.22325\n",
      "Epoch: 5/30 Iteration: 20 Train loss: 0.19019\n",
      "Epoch: 5/30 Iteration: 25 Train loss: 0.18561\n",
      "Epoch: 5/30 Iteration: 30 Train loss: 0.17231\n",
      "Epoch: 6/30 Iteration: 5 Train loss: 0.16676\n",
      "Epoch: 6/30 Iteration: 10 Train loss: 0.14349\n",
      "Epoch: 6/30 Iteration: 15 Train loss: 0.16166\n",
      "Epoch: 6/30 Iteration: 20 Train loss: 0.13186\n",
      "Epoch: 6/30 Iteration: 25 Train loss: 0.12617\n",
      "Epoch: 6/30 Iteration: 30 Train loss: 0.12824\n",
      "Epoch: 7/30 Iteration: 5 Train loss: 0.15193\n",
      "Epoch: 7/30 Iteration: 10 Train loss: 0.09854\n",
      "Epoch: 7/30 Iteration: 15 Train loss: 0.12516\n",
      "Epoch: 7/30 Iteration: 20 Train loss: 0.10592\n",
      "Epoch: 7/30 Iteration: 25 Train loss: 0.10232\n",
      "Epoch: 7/30 Iteration: 30 Train loss: 0.10005\n",
      "Epoch: 8/30 Iteration: 5 Train loss: 0.10211\n",
      "Epoch: 8/30 Iteration: 10 Train loss: 0.07824\n",
      "Epoch: 8/30 Iteration: 15 Train loss: 0.10430\n",
      "Epoch: 8/30 Iteration: 20 Train loss: 0.09055\n",
      "Epoch: 8/30 Iteration: 25 Train loss: 0.08028\n",
      "Epoch: 8/30 Iteration: 30 Train loss: 0.08304\n",
      "Epoch: 9/30 Iteration: 5 Train loss: 0.07773\n",
      "Epoch: 9/30 Iteration: 10 Train loss: 0.05934\n",
      "Epoch: 9/30 Iteration: 15 Train loss: 0.07816\n",
      "Epoch: 9/30 Iteration: 20 Train loss: 0.06131\n",
      "Epoch: 9/30 Iteration: 25 Train loss: 0.05720\n",
      "Epoch: 9/30 Iteration: 30 Train loss: 0.07185\n",
      "Epoch: 10/30 Iteration: 5 Train loss: 0.06652\n",
      "Epoch: 10/30 Iteration: 10 Train loss: 0.04071\n",
      "Epoch: 10/30 Iteration: 15 Train loss: 0.06262\n",
      "Epoch: 10/30 Iteration: 20 Train loss: 0.06394\n",
      "Epoch: 10/30 Iteration: 25 Train loss: 0.05326\n",
      "Epoch: 10/30 Iteration: 30 Train loss: 0.05756\n",
      "Epoch: 11/30 Iteration: 5 Train loss: 0.05729\n",
      "Epoch: 11/30 Iteration: 10 Train loss: 0.03845\n",
      "Epoch: 11/30 Iteration: 15 Train loss: 0.06112\n",
      "Epoch: 11/30 Iteration: 20 Train loss: 0.05366\n",
      "Epoch: 11/30 Iteration: 25 Train loss: 0.05404\n",
      "Epoch: 11/30 Iteration: 30 Train loss: 0.05866\n",
      "Epoch: 12/30 Iteration: 5 Train loss: 0.05454\n",
      "Epoch: 12/30 Iteration: 10 Train loss: 0.02978\n",
      "Epoch: 12/30 Iteration: 15 Train loss: 0.05329\n",
      "Epoch: 12/30 Iteration: 20 Train loss: 0.05255\n",
      "Epoch: 12/30 Iteration: 25 Train loss: 0.04962\n",
      "Epoch: 12/30 Iteration: 30 Train loss: 0.05468\n",
      "Epoch: 13/30 Iteration: 5 Train loss: 0.05069\n",
      "Epoch: 13/30 Iteration: 10 Train loss: 0.02872\n",
      "Epoch: 13/30 Iteration: 15 Train loss: 0.05554\n",
      "Epoch: 13/30 Iteration: 20 Train loss: 0.04678\n",
      "Epoch: 13/30 Iteration: 25 Train loss: 0.05490\n",
      "Epoch: 13/30 Iteration: 30 Train loss: 0.05845\n",
      "Epoch: 14/30 Iteration: 5 Train loss: 0.04840\n",
      "Epoch: 14/30 Iteration: 10 Train loss: 0.02983\n",
      "Epoch: 14/30 Iteration: 15 Train loss: 0.05505\n",
      "Epoch: 14/30 Iteration: 20 Train loss: 0.04856\n",
      "Epoch: 14/30 Iteration: 25 Train loss: 0.05678\n",
      "Epoch: 14/30 Iteration: 30 Train loss: 0.04915\n",
      "Epoch: 15/30 Iteration: 5 Train loss: 0.05060\n",
      "Epoch: 15/30 Iteration: 10 Train loss: 0.03159\n",
      "Epoch: 15/30 Iteration: 15 Train loss: 0.05744\n",
      "Epoch: 15/30 Iteration: 20 Train loss: 0.05315\n",
      "Epoch: 15/30 Iteration: 25 Train loss: 0.05156\n",
      "Epoch: 15/30 Iteration: 30 Train loss: 0.05783\n",
      "Epoch: 16/30 Iteration: 5 Train loss: 0.05482\n",
      "Epoch: 16/30 Iteration: 10 Train loss: 0.02407\n",
      "Epoch: 16/30 Iteration: 15 Train loss: 0.05556\n",
      "Epoch: 16/30 Iteration: 20 Train loss: 0.04676\n",
      "Epoch: 16/30 Iteration: 25 Train loss: 0.04602\n",
      "Epoch: 16/30 Iteration: 30 Train loss: 0.05171\n",
      "Epoch: 17/30 Iteration: 5 Train loss: 0.04599\n",
      "Epoch: 17/30 Iteration: 10 Train loss: 0.02705\n",
      "Epoch: 17/30 Iteration: 15 Train loss: 0.05365\n",
      "Epoch: 17/30 Iteration: 20 Train loss: 0.04498\n",
      "Epoch: 17/30 Iteration: 25 Train loss: 0.04493\n",
      "Epoch: 17/30 Iteration: 30 Train loss: 0.04562\n",
      "Epoch: 18/30 Iteration: 5 Train loss: 0.04648\n",
      "Epoch: 18/30 Iteration: 10 Train loss: 0.02842\n",
      "Epoch: 18/30 Iteration: 15 Train loss: 0.05593\n",
      "Epoch: 18/30 Iteration: 20 Train loss: 0.04994\n",
      "Epoch: 18/30 Iteration: 25 Train loss: 0.04909\n",
      "Epoch: 18/30 Iteration: 30 Train loss: 0.04539\n",
      "Epoch: 19/30 Iteration: 5 Train loss: 0.04461\n",
      "Epoch: 19/30 Iteration: 10 Train loss: 0.02177\n",
      "Epoch: 19/30 Iteration: 15 Train loss: 0.05310\n",
      "Epoch: 19/30 Iteration: 20 Train loss: 0.05068\n",
      "Epoch: 19/30 Iteration: 25 Train loss: 0.04602\n",
      "Epoch: 19/30 Iteration: 30 Train loss: 0.04539\n",
      "Epoch: 20/30 Iteration: 5 Train loss: 0.04503\n",
      "Epoch: 20/30 Iteration: 10 Train loss: 0.01957\n",
      "Epoch: 20/30 Iteration: 15 Train loss: 0.05470\n",
      "Epoch: 20/30 Iteration: 20 Train loss: 0.04619\n",
      "Epoch: 20/30 Iteration: 25 Train loss: 0.04357\n",
      "Epoch: 20/30 Iteration: 30 Train loss: 0.04460\n",
      "Epoch: 21/30 Iteration: 5 Train loss: 0.04439\n",
      "Epoch: 21/30 Iteration: 10 Train loss: 0.01890\n",
      "Epoch: 21/30 Iteration: 15 Train loss: 0.05328\n",
      "Epoch: 21/30 Iteration: 20 Train loss: 0.04580\n",
      "Epoch: 21/30 Iteration: 25 Train loss: 0.04367\n",
      "Epoch: 21/30 Iteration: 30 Train loss: 0.04414\n",
      "Epoch: 22/30 Iteration: 5 Train loss: 0.04045\n",
      "Epoch: 22/30 Iteration: 10 Train loss: 0.01904\n",
      "Epoch: 22/30 Iteration: 15 Train loss: 0.05602\n",
      "Epoch: 22/30 Iteration: 20 Train loss: 0.04697\n",
      "Epoch: 22/30 Iteration: 25 Train loss: 0.04286\n",
      "Epoch: 22/30 Iteration: 30 Train loss: 0.04285\n",
      "Epoch: 23/30 Iteration: 5 Train loss: 0.04105\n",
      "Epoch: 23/30 Iteration: 10 Train loss: 0.02037\n",
      "Epoch: 23/30 Iteration: 15 Train loss: 0.05419\n",
      "Epoch: 23/30 Iteration: 20 Train loss: 0.04733\n",
      "Epoch: 23/30 Iteration: 25 Train loss: 0.03788\n",
      "Epoch: 23/30 Iteration: 30 Train loss: 0.04211\n",
      "Epoch: 24/30 Iteration: 5 Train loss: 0.04078\n",
      "Epoch: 24/30 Iteration: 10 Train loss: 0.01858\n",
      "Epoch: 24/30 Iteration: 15 Train loss: 0.05332\n",
      "Epoch: 24/30 Iteration: 20 Train loss: 0.04700\n",
      "Epoch: 24/30 Iteration: 25 Train loss: 0.04671\n",
      "Epoch: 24/30 Iteration: 30 Train loss: 0.04412\n",
      "Epoch: 25/30 Iteration: 5 Train loss: 0.04398\n",
      "Epoch: 25/30 Iteration: 10 Train loss: 0.02188\n",
      "Epoch: 25/30 Iteration: 15 Train loss: 0.05133\n",
      "Epoch: 25/30 Iteration: 20 Train loss: 0.04634\n",
      "Epoch: 25/30 Iteration: 25 Train loss: 0.04398\n",
      "Epoch: 25/30 Iteration: 30 Train loss: 0.05079\n",
      "Epoch: 26/30 Iteration: 5 Train loss: 0.03971\n",
      "Epoch: 26/30 Iteration: 10 Train loss: 0.02482\n",
      "Epoch: 26/30 Iteration: 15 Train loss: 0.05220\n",
      "Epoch: 26/30 Iteration: 20 Train loss: 0.04052\n",
      "Epoch: 26/30 Iteration: 25 Train loss: 0.03949\n",
      "Epoch: 26/30 Iteration: 30 Train loss: 0.04306\n",
      "Epoch: 27/30 Iteration: 5 Train loss: 0.04736\n",
      "Epoch: 27/30 Iteration: 10 Train loss: 0.03320\n",
      "Epoch: 27/30 Iteration: 15 Train loss: 0.05783\n",
      "Epoch: 27/30 Iteration: 20 Train loss: 0.04620\n",
      "Epoch: 27/30 Iteration: 25 Train loss: 0.05315\n",
      "Epoch: 27/30 Iteration: 30 Train loss: 0.04543\n",
      "Epoch: 28/30 Iteration: 5 Train loss: 0.04270\n",
      "Epoch: 28/30 Iteration: 10 Train loss: 0.03357\n",
      "Epoch: 28/30 Iteration: 15 Train loss: 0.05830\n",
      "Epoch: 28/30 Iteration: 20 Train loss: 0.05805\n",
      "Epoch: 28/30 Iteration: 25 Train loss: 0.03812\n",
      "Epoch: 28/30 Iteration: 30 Train loss: 0.04463\n",
      "Epoch: 29/30 Iteration: 5 Train loss: 0.04979\n",
      "Epoch: 29/30 Iteration: 10 Train loss: 0.02340\n",
      "Epoch: 29/30 Iteration: 15 Train loss: 0.05293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/30 Iteration: 20 Train loss: 0.04550\n",
      "Epoch: 29/30 Iteration: 25 Train loss: 0.04189\n",
      "Epoch: 29/30 Iteration: 30 Train loss: 0.04413\n",
      "Training Completed\n",
      "Total Time Taken: 360.4311866760254 sec\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "global_train_acc = []\n",
    "global_test_acc = []\n",
    "\n",
    "global_train_loss = []\n",
    "global_test_loss = []\n",
    "\n",
    "global_val_imdb_loss = []\n",
    "global_val_chat_loss = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "min_loss = 1.0\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for e in range(n_epochs):\n",
    "    state = sess.run(initial_state)\n",
    "    iteration = 1\n",
    "    loss_=0.0\n",
    "    temp_train_loss = []\n",
    "    for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "        feed = {X: x, Y: y, initial_state: state}\n",
    "\n",
    "        state, loss_,  _ = sess.run([final_state, loss, optimizer], feed_dict=feed)\n",
    "\n",
    "        if iteration%5==0:\n",
    "            print(\"Epoch: {}/{}\".format(e, n_epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Train loss: {:.5f}\".format(loss_))\n",
    "        temp_train_loss.append(loss_)\n",
    "        \n",
    "        '''\n",
    "        if loss_<min_loss:\n",
    "            min_loss = loss_\n",
    "            save_path = saver.save(sess, \"/model.ckpt\")\n",
    "        '''\n",
    "        iteration+=1\n",
    "    global_train_loss.append(np.mean(temp_train_loss))\n",
    "    \n",
    "    ##Validation loss IMDB\n",
    "    val_loss = []\n",
    "    val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for x, y in get_batches(reviews_val, labels_val, batch_size):\n",
    "        feed = {X: x,\n",
    "                Y: y,\n",
    "                initial_state: val_state}\n",
    "        loss_, val_state = sess.run([loss, final_state], feed_dict=feed)\n",
    "        val_loss.append(loss_)\n",
    "    global_val_imdb_loss.append(np.mean(val_loss))\n",
    "    \n",
    "    ##Validation loss Chat\n",
    "    val_loss = []\n",
    "    val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for x, y in get_batches(dialogues_val, sentiments_val, batch_size):\n",
    "        feed = {X: x,\n",
    "                Y: y,\n",
    "                initial_state: val_state}\n",
    "        loss_, val_state = sess.run([loss, final_state], feed_dict=feed)\n",
    "        val_loss.append(loss_)\n",
    "    global_val_chat_loss.append(np.mean(val_loss))\n",
    "    \n",
    "#sess.close()\n",
    "    \n",
    "print('Training Completed')\n",
    "print('Total Time Taken: '+str(time.time()-start_time)+' sec' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc for movie reviews: 0.73600\n",
      "Test acc for chat : 0.59800\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"/model.ckpt\")\n",
    "'''\n",
    "\n",
    "test_acc = []\n",
    "test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "for ii, (x, y) in enumerate(get_batches(reviews_test, labels_test, batch_size), 1):\n",
    "    feed = {X: x,Y: y,initial_state: test_state, keep_prob: 1}\n",
    "\n",
    "    batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "    test_acc.append(batch_acc)\n",
    "print(\"Test acc for movie reviews: {:.5f}\".format(np.mean(test_acc)))\n",
    "\n",
    "test_acc = []\n",
    "test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "for ii, (x, y) in enumerate(get_batches(dialogues_test, sentiments_test, batch_size), 1):\n",
    "    feed = {X: x,Y: y,initial_state: test_state, keep_prob: 1}\n",
    "\n",
    "    batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "    test_acc.append(batch_acc)\n",
    "print(\"Test acc for chat : {:.5f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Test Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32195783, 0.29884675, 0.28273973, 0.25540069, 0.23617926, 0.20261253, 0.14270063, 0.10868407, 0.091201462, 0.071786895, 0.062545739, 0.057211924, 0.054398336, 0.053318668, 0.05258444, 0.051699281, 0.049349293, 0.047395498, 0.046660595, 0.046678189, 0.044291239, 0.043390792, 0.0426699, 0.041656047, 0.042603947, 0.04330194, 0.043237947, 0.047744997, 0.048177555, 0.043771639]\n",
      "[0.25003272, 0.2527003, 0.26595965, 0.27262005, 0.28181821, 0.20710063, 0.19411764, 0.21110389, 0.21291578, 0.2088474, 0.2066139, 0.22041114, 0.21599157, 0.22860813, 0.23370969, 0.22160463, 0.22585921, 0.22510557, 0.22210319, 0.21999626, 0.22337258, 0.22198172, 0.22888088, 0.23899105, 0.23830895, 0.24449888, 0.23063929, 0.24003787, 0.22933006, 0.23404746]\n",
      "[0.80078173, 0.8000378, 0.79934043, 0.79984182, 0.80880016, 0.71073753, 0.65490556, 0.65837491, 0.65273929, 0.65136904, 0.67185026, 0.66672599, 0.64787459, 0.64951217, 0.66423476, 0.65197384, 0.64613032, 0.65200555, 0.63907182, 0.63725513, 0.64364433, 0.63824135, 0.63650393, 0.64606512, 0.63840336, 0.62583834, 0.62986225, 0.64868331, 0.63930678, 0.63143384]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e2ff29278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(global_train_loss)\n",
    "print(global_val_imdb_loss)\n",
    "print(global_val_chat_loss)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.ion()\n",
    "x = range(30) \n",
    "plt.axis([0,len(x)+5,0,1])\n",
    "plt.plot(x,global_train_loss,'go',x,global_val_imdb_loss,'ro',x,global_val_chat_loss,'bo')\n",
    "plt.show()\n",
    "\n",
    "#print('Minimum Testing Loss: '+str(np.min(global_test_loss)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
